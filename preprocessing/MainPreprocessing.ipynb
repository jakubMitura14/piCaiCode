{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "from zipfile import ZipFile\n",
    "from zipfile import BadZipFile\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from functools import partial\n",
    "import Standardize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## managment of files \n",
    "managment of files is done via managePicaiFiles.sh - create directories download and unpack files saves basic metadata and do simple metadata preprocessing\n",
    "sh managePicaiFiles.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization\n",
    "primary preprocessing - removing ouliers, put values between 0 and 255 bias field correction Nyul standarization binarizing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       /home/sliceruser/data/orig/10000/10000_1000000...\n",
       "1       /home/sliceruser/data/orig/10001/10001_1000001...\n",
       "2       /home/sliceruser/data/orig/10002/10002_1000002...\n",
       "3       /home/sliceruser/data/orig/10003/10003_1000003...\n",
       "4       /home/sliceruser/data/orig/10004/10004_1000004...\n",
       "                              ...                        \n",
       "1495    /home/sliceruser/data/orig/11471/11471_1001495...\n",
       "1496    /home/sliceruser/data/orig/11472/11472_1001496...\n",
       "1497    /home/sliceruser/data/orig/11473/11473_1001497...\n",
       "1498    /home/sliceruser/data/orig/11474/11474_1001498...\n",
       "1499    /home/sliceruser/data/orig/11475/11475_1001499...\n",
       "Name: sag, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "df['sag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newwD\n",
      "newwD\n",
      "newwD\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10001/10001_1000001_t2w.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10002/10002_1000002_t2w.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10000/10000_1000000_t2w.mha\n",
      "fitting normalizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sliceruser/Slicer/lib/Python/lib/python3.9/site-packages/intensity_normalization/normalize/base.py:89: UserWarning: Data contains negative values; skull-stripped functionality assumes the foreground is all positive. Provide the brain mask if otherwise.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10001/10001_1000001_t2w.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10000/10000_1000000_t2w.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10002/10002_1000002_t2w.mha\n",
      "newwD\n",
      "newwD\n",
      "newwD\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10002/10002_1000002_adc.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10000/10000_1000000_adc.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10001/10001_1000001_adc.mha\n",
      "fitting normalizer\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10000/10000_1000000_adc.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10001/10001_1000001_adc.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10002/10002_1000002_adc.mha\n",
      "newwD\n",
      "newwD\n",
      "newwD\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10002/10002_1000002_cor.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10001/10001_1000001_cor.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10000/10000_1000000_cor.mha\n",
      "fitting normalizer\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10002/10002_1000002_cor.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10001/10001_1000001_cor.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10000/10000_1000000_cor.mha\n",
      "newwD\n",
      "newwD\n",
      "newwD\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10002/10002_1000002_hbv.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10000/10000_1000000_hbv.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10001/10001_1000001_hbv.mha\n",
      "fitting normalizer\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10000/10000_1000000_hbv.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10001/10001_1000001_hbv.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10002/10002_1000002_hbv.mha\n",
      "newwD\n",
      "newwD\n",
      "newwD\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10001/10001_1000001_sag.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10002/10002_1000002_sag.mha\n",
      "biasFieldCorrect /home/sliceruser/data/orig/10000/10000_1000000_sag.mha\n",
      "fitting normalizer\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10000/10000_1000000_sag.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10001/10001_1000001_sag.mha\n",
      "standardizeFromPathAndOverwrite /home/sliceruser/data/orig/10002/10002_1000002_sag.mha\n"
     ]
    }
   ],
   "source": [
    "import Standardize\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "numRows=3\n",
    "Standardize.iterateAndchangeLabelToOnes(numRows,df)\n",
    "for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]:\n",
    "    Standardize.iterateAndStandardize(keyWord,numRows,df)\n",
    "\n",
    "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Unires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frst we need to save files as nifti- we will keep the same folder structure just alternate file type\n",
    "def saveAsNifti(path):\n",
    "    image=sitk.ReadImage(path)\n",
    "    newPath = path.replace(\".mha\",\".nii.gz\")\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(newPath)\n",
    "    writer.Execute(image)\n",
    "    return newPath\n",
    "\n",
    "for keyWord in ['t2w','adc', 'cor','hbv','sag','reSampledPath']:\n",
    "    train_patientsPaths=df[keyWord].dropna().astype('str').to_numpy()\n",
    "    train_patientsPaths=list(filter(lambda path: len(path)>2 ,train_patientsPaths))\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(saveAsNifti,train_patientsPaths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:nitorch uses its non-compiled backend (TS). Some algorithms may be slow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "262.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unires.struct import settings as s\n",
    "from unires.run import preproc\n",
    "\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "numb= 19\n",
    "\n",
    "pathT2w=df['t2w'].to_numpy()[numb]\n",
    "pathCor=df['cor'].to_numpy()[numb]\n",
    "pathSag=df['sag'].to_numpy()[numb]\n",
    "pathHbv=df['hbv'].to_numpy()[numb]\n",
    "pathAdc=df['adc'].to_numpy()[numb]\n",
    "pathLabel=df['reSampledPath'].to_numpy()[numb]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def saveAsNifti(path):\n",
    "    image=sitk.ReadImage(path)\n",
    "    newPath = path.replace(\".mha\",\".nii.gz\")\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(newPath)\n",
    "    writer.Execute(image)\n",
    "    return newPath\n",
    "pathT2wNew = saveAsNifti(pathT2w)\n",
    "pathCorNew = saveAsNifti(pathCor)\n",
    "pathSagNew=saveAsNifti(pathSag)\n",
    "pathHbvNew=saveAsNifti(pathHbv)\n",
    "pathAdcNew=saveAsNifti(pathAdc)\n",
    "pathLabelNew=saveAsNifti(pathLabel)\n",
    "\n",
    "\n",
    "df['isAnythingInAnnotated'].to_numpy()[numb]\n",
    "\n",
    "#image1=sitk.ReadImage(pathT2w)\n",
    "# pathT2w.replace(\".mha\",\".nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sliceruser/data/orig/10019/10019_1000019_t2w.nii.gz\n",
      "/home/sliceruser/data/orig/10019/10019_1000019_cor.nii.gz\n",
      "/home/sliceruser/data/orig/10019/10019_1000019_sag.nii.gz\n"
     ]
    }
   ],
   "source": [
    "print(pathT2wNew)\n",
    "print(pathCorNew)\n",
    "print(pathSagNew)\n",
    "sett=s()\n",
    "sett.device='cpu' #TODO() remove\n",
    "#sett.do_atlas_align='false'\n",
    "sett.do_atlas_align=False\n",
    "#sett.cgs_max_iter=3#TODO() remove\n",
    "sett.max_iter=110#TODO() remove\n",
    "sett.dir_out = '/home/sliceruser/data/Unires'\n",
    "sett.label= pathLabelNew\n",
    "\n",
    "\n",
    "# in command line\n",
    "# unires /home/sliceruser/data/orig/10000/10000_1000000_t2w.nii.gz /home/sliceruser/data/orig/10000/10000_1000000_cor.nii.gz /home/sliceruser/data/orig/10000/10000_1000000_sag.nii.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preproc([pathT2wNew, pathCorNew,pathSagNew,pathHbvNew,pathAdcNew ],sett=sett)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='/home/sliceruser/data/Unires/ur_10000_1000000_cor.nii.gz'\n",
    "b='/home/sliceruser/data/Unires/ur_10000_1000000_sag.nii.gz'\n",
    "c='/home/sliceruser/data/Unires/ur_10000_1000000_t2w.nii.gz'\n",
    "image=sitk.ReadImage(c)\n",
    "image.GetSpacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(203, 251, 252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all of the spacings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# if using a Jupyter notebook, includue:\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t2w_spac_x': (0.234375, 0.78125, 0.5, 0.5),\n",
       " 't2w_spac_y': (0.234375, 0.78125, 0.5, 0.5),\n",
       " 't2w_spac_z': (2.200000060372773, 5.000000066297424, 3.6, 3.0),\n",
       " 'adc_spac_x': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
       " 'adc_spac_y': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
       " 'adc_spac_z': (2.999998832062665, 5.000000232855622, 4.0, 3.0),\n",
       " 'cor_spac_x': (0.286756128073, 0.9765625, 0.6, 0.6),\n",
       " 'cor_spac_y': (0.286756128073, 0.9765625, 0.6, 0.6),\n",
       " 'cor_spac_z': (0.9999999999999636, 4.49999998188102, 2.7, 3.0),\n",
       " 'hbv_spac_x': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
       " 'hbv_spac_y': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
       " 'hbv_spac_z': (2.999998832062665, 5.000000232855622, 4.0, 3.0),\n",
       " 'sag_spac_x': (0.254629641771, 0.78125, 0.5, 0.6),\n",
       " 'sag_spac_y': (0.254629641771, 0.78125, 0.5, 0.6),\n",
       " 'sag_spac_z': (2.999999882111563, 4.000404550206146, 3.5, 3.6)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "\n",
    "\"\"\"\n",
    "looking through all valid spacings (if it si invalid it goes below 0)\n",
    "and displaying minimal maximal and rounded mean spacing and median\n",
    "in my case median and mean values are close - and using the median values will lead to a bit less interpolations later\n",
    "\"\"\"\n",
    "spacingDict={}\n",
    "for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]: \n",
    "    for addedKey in ['_spac_x','_spac_y','_spac_z']:   \n",
    "        colName = keyWord+addedKey\n",
    "        liist = list(filter(lambda it: it>0 ,df[colName].to_numpy() ))\n",
    "        minn=np.min(liist)                \n",
    "        maxx=np.max(liist)\n",
    "        meanRounded = round((minn+maxx)/2,1)\n",
    "        medianRounded = round(np.median(liist),1)\n",
    "        spacingDict[colName]=(minn,maxx,meanRounded,medianRounded)\n",
    "\n",
    "spacingDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now idea is to iteratively transform images into uniform spacing here i will choose median of the data as the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KevinSR import mask_interpolation, SOUP_GAN\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import ndimage, interpolate\n",
    "from scipy.ndimage import zoom\n",
    "from KevinSR import SOUP_GAN\n",
    "import monai\n",
    "import SimpleITK as sitk\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def superSampleinZ(image, currentSpacZ, targetSpacZ):\n",
    "    \"\"\"\n",
    "    perform supersampling in Z direction\n",
    "    currentSpac, targetSpac - current and target z spacing - float numbers\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def subsample(image,currentSpac, targetSpac):\n",
    "    \"\"\"\n",
    "    perform subsampling to targetSpac\n",
    "    \"\"\"    \n",
    "    pass\n",
    "\n",
    "\n",
    "def transformSpacingTo(path, targetSpac):\n",
    "    \"\"\"\n",
    "    transforming spacing of the image which path is supplied to target spacing values\n",
    "    upsampling will be performed with SOUP-GAN  and downsampling will be done via simple itk bicubic interpolation\n",
    "    \"\"\"\n",
    "    image = sitk.ReadImage(path)\n",
    "    spac = image.GetSpacing()\n",
    "    for axis in [0,1,2]:\n",
    "        if(targetSpac[axis]>spac[axis]):\n",
    "            #if target spacing is bigger we need to subsample\n",
    "            spacGoal=deepcopy(spac)\n",
    "            spacGoal[axis]=\n",
    "            subsample(image, )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from KevinSR import mask_interpolation, SOUP_GAN\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from scipy import ndimage, interpolate\n",
    "# from scipy.ndimage import zoom\n",
    "# from KevinSR import SOUP_GAN\n",
    "# import monai\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "\n",
    "# path = df['t2w'].to_numpy()[0]\n",
    "\n",
    "\n",
    "\n",
    "# spacingX_a= 0.234375\n",
    "# spacingX_b= 0.78125\n",
    "\n",
    "# spacingY_a= 0.234375\n",
    "# spacingY_b= 0.78125\n",
    "\n",
    "# spacingZ_a= 2.200000060372773\n",
    "# spacingZ_b= 5.000000066297424\n",
    "\n",
    "# img = sitk.ReadImage(path)\n",
    "# image1 = sitk.ReadImage(path)\n",
    "\n",
    "# thicks_ori = sitk.GetArrayFromImage(image1)\n",
    "# Z_FAC = spacingZ_b/spacingZ_a\n",
    "\n",
    "# # Call the SR interpolation tool from KevinSR\n",
    "# thins_gen = SOUP_GAN(thicks_ori, Z_FAC, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #recreating image keeping relevant metadata\n",
    "# image = sitk.GetImageFromArray(data)\n",
    "# image.SetSpacing(image1.GetSpacing())\n",
    "# image.SetOrigin(image1.GetOrigin())\n",
    "# image.SetDirection(image1.GetDirection())\n",
    "\n",
    "\n",
    "\n",
    "# #Example of thick-to-thin type of preprocessing\n",
    "# pre_slices = 27\n",
    "# post_slices = 135\n",
    "\n",
    "\n",
    "# # thicks_ori = load_data(DIR_dicom)\n",
    "# # Z_FAC = post_slices/pre_slices # Sampling factor in Z direction\n",
    "\n",
    "# # thicks_ori = rescale_img(thicks_ori, max_val= 10000)\n",
    "\n",
    "# # thins = zoom(thicks_ori, (1,1,Z_FAC))\n",
    "# # thins_raw = zoom(thicks_ori, (1,1,Z_FAC),order=0)\n",
    "\n",
    "# # # Call the SR interpolation tool from KevinSR\n",
    "# # thins_gen = SOUP_GAN(thicks_ori, Z_FAC, 0)\n",
    "\n",
    "# # # Plot the original thick slices, thin slices by TCI and SR generated slices. \n",
    "# # plot_scans(thins_raw, thins, thins_gen, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]: \n",
    "#     for addedKey in ['_sizz_x','_sizz_y','_sizz_z','_spac_x'\n",
    "#                         ,'_spac_y','_spac_z'\n",
    "#                         ,'_orig_x','_orig_x','_orig_x']:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupwise registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/SuperElastix/SimpleElastix/issues/255\n",
    "with ants \n",
    "https://github.com/SunYH66/adn-master/blob/d69a73e2f9cf2a4472c1d97f7347677b1947543a/adn/utils/preprocessing.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
