{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Standardize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/media/jakub/New Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000000?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000000?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000000?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mStandardize\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Standardize'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "from zipfile import ZipFile\n",
    "from zipfile import BadZipFile\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from functools import partial\n",
    "import Standardize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## managment of files \n",
    "managment of files is done via managePicaiFiles.sh - create directories download and unpack files saves basic metadata and do simple metadata preprocessing\n",
    "sh managePicaiFiles.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization\n",
    "primary preprocessing - removing ouliers, put values between 0 and 255 bias field correction Nyul standarization binarizing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathBaselineImage ='/home/sliceruser/data/10001/10001_1000001_t2w.mha'\n",
    "from __future__ import print_function\n",
    "import SimpleITK as sitk\n",
    "from os import listdir\n",
    "from scipy.interpolate import interp1d\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import isdir,join,exists,split,dirname,basename\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def removeOutliersBiasFieldCorrect(path,numberOfStandardDeviations = 4):\n",
    "    \"\"\"\n",
    "    all taken from https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/Main_Preprocessing.py\n",
    "    my modification that instead of histogram usage for outliers I use the standard deviations\n",
    "    path - path to file to be processed\n",
    "    numberOfStandardDeviations- osed to define outliers\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    image1 = sitk.ReadImage(path)\n",
    "    data = sitk.GetArrayFromImage(image1)\n",
    "    # shift the data up so that all intensity values turn positive\n",
    "    stdd = np.std(data)*5\n",
    "    median = np.median(data)\n",
    "    data = np.clip(data, median-numberOfStandardDeviations*stdd, median+numberOfStandardDeviations*stdd)\n",
    "    data -= np.min(data)\n",
    "    #TO normalize an image by mapping its [Min,Max] into the interval [0,255]\n",
    "    N=255\n",
    "    data=N*(data+600)/2000\n",
    "\n",
    "    #recreating image keeping relevant metadata\n",
    "    image = sitk.GetImageFromArray(data)\n",
    "    image.SetSpacing(image1.GetSpacing())\n",
    "    image.SetOrigin(image1.GetOrigin())\n",
    "    image.SetDirection(image1.GetDirection())\n",
    "    #bias field normalization\n",
    "    maskImage = sitk.OtsuThreshold(image, 0, 1, 200)\n",
    "    inputImage = sitk.Cast(image, sitk.sitkFloat32)\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    # numberFittingLevels = 4\n",
    "    imageB = corrector.Execute(inputImage, maskImage)\n",
    "    imageB.SetSpacing(image.GetSpacing())\n",
    "    imageB.SetOrigin(image.GetOrigin())\n",
    "    imageB.SetDirection(image.GetDirection())\n",
    "    return imageB\n",
    "\n",
    "#####################\n",
    "#taken and adapted from https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/image/Nyul_preprocessing.py\n",
    "# !/usr/bin/env python3\n",
    "# -------------------------------------------------------------------------------\n",
    "# Author: Samira Masoudi\n",
    "# Date:   11.07.2019\n",
    "# Based on Nyul et al. 2000: New variants of a method of MRI scale standardization\n",
    "# Full version can be found at https://github.com/sergivalverde/MRI_intensity_normalization\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "def tic():\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "\n",
    "def toc():\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        print\n",
    "        \"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\"\n",
    "    else:\n",
    "        print\n",
    "        \"Toc: start time not set\"\n",
    "\n",
    "\n",
    "def getCdf(hist):\n",
    "    \"\"\"\n",
    "        Given a histogram, it returns the cumulative distribution function.\n",
    "    \"\"\"\n",
    "    aux = np.cumsum(hist)\n",
    "    aux = aux / aux[-1] * 100\n",
    "    return aux\n",
    "\n",
    "\n",
    "def getPercentile(cdf, bins, perc):\n",
    "    \"\"\"\n",
    "        Given a cumulative distribution function obtained from a histogram,\n",
    "        (where 'bins' are the x values of the histogram and 'cdf' is the\n",
    "        cumulative distribution function of the original histogram), it returns\n",
    "        the x center value for the bin index corresponding to the given percentile,\n",
    "        and the bin index itself.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            import numpy as np\n",
    "            hist = np.array([204., 1651., 2405., 1972., 872., 1455.])\n",
    "            bins = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "\n",
    "            cumHist = getCdf(hist)\n",
    "            print cumHist\n",
    "            val, bin = getPercentile(cumHist, bins, 50)\n",
    "\n",
    "            print \"Val = \" + str(val)\n",
    "            print \"Bin = \" + str(bin)\n",
    "\n",
    "    \"\"\"\n",
    "    b = len(bins[cdf <= perc])\n",
    "    return bins[b] + ((bins[1] - bins[0]) / 2)\n",
    "\n",
    "\n",
    "\n",
    "def getLandmarks(image, mask=None, showLandmarks=False,nbins=1024, pLow=1, pHigh=99,numPoints=10):\n",
    "        \"\"\"\n",
    "            This Private function obtain the landmarks for a given image and returns them\n",
    "            in a list like:\n",
    "                [lm_pLow, lm_perc1, lm_perc2, ... lm_perc_(numPoints-1), lm_pHigh] (lm means landmark)\n",
    "\n",
    "            :param image    SimpleITK image for which the landmarks are computed.\n",
    "            :param mask     [OPTIONAL] SimpleITK image containing a mask. If provided, the histogram will be computed\n",
    "                                    taking into account only the voxels where mask > 0.\n",
    "            :param showLandmarks    Plot the landmarks using matplotlib on top of the histogram.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data = sitk.GetArrayFromImage(image)\n",
    "        if mask is None:\n",
    "            # Calculate useful statistics\n",
    "            stats = sitk.StatisticsImageFilter()\n",
    "            stats.Execute(image)\n",
    "            mean =stats.GetMean()\n",
    "\n",
    "            # Compute the image histogram\n",
    "            histo, bins = np.histogram(data.flatten(), nbins)\n",
    "\n",
    "            # Calculate the cumulative distribution function of the original histogram\n",
    "            cdfOriginal = getCdf(histo)\n",
    "\n",
    "            # Truncate the histogram (put 0 to those values whose intensity is less than the mean)\n",
    "            # so that only the foreground values are considered for the landmark learning process\n",
    "            histo[bins[:-1] < mean] = 0.0\n",
    "        # else:\n",
    "        #     # Calculate useful statistics\n",
    "        #     dataMask = sitk.GetArrayFromImage(mask)\n",
    "        #\n",
    "        #     # Compute the image histogram\n",
    "        #     histo, bins = np.histogram(data[dataMask > 0].flatten(), nbins, normed=True)\n",
    "        #\n",
    "        #     # Calculate the cumulative distribution function of the original histogram\n",
    "        #     cdfOriginal = getCdf(histo)\n",
    "\n",
    "        # Calculate the cumulative distribution function of the truncated histogram, where outliers are removed\n",
    "        cdfTruncated = getCdf(histo)\n",
    "\n",
    "        # Generate the percentile landmarks for  m_i\n",
    "        perc = [x for x in range(0, 100, 100 // numPoints)]\n",
    "        # Remove the first landmark that will always correspond to 0\n",
    "        perc = perc[1:]\n",
    "\n",
    "        # Generate the landmarks. Note that those corresponding to pLow and pHigh (at the beginning and the\n",
    "        # end of the list of landmarks) are generated from the cdfOriginal, while the ones\n",
    "        # corresponding to the percentiles are generated from cdfTruncated, meaning that only foreground intensities\n",
    "        # are considered.\n",
    "\n",
    "        landmarks = [getPercentile(cdfOriginal, bins[:-1], pLow)] + [getPercentile(cdfTruncated, bins[:-1], x) for x in perc] + [getPercentile(cdfOriginal, bins[:-1], pHigh)]\n",
    "        # landmarks_org =  [getPercentile(cdfOriginal, bins[:-1], x) for x in [pLow]+perc+[pHigh]]\n",
    "        return landmarks\n",
    "\n",
    "def  landmarksSanityCheck(landmarks):\n",
    "        Flag=True\n",
    "        if not (np.unique(landmarks).size == len(landmarks)):\n",
    "            for i in range(1, len(landmarks) - 1):\n",
    "                if landmarks[i] == landmarks[i + 1]:\n",
    "                    landmarks[i] = (landmarks[i - 1] + landmarks[i + 1]) / 2.0\n",
    "\n",
    "                print( \"WARNING: Fixing duplicate landmark.\")\n",
    "\n",
    "            if not (np.unique(landmarks).size == len(landmarks)):\n",
    "                raise Exception('ERROR NyulNormalizer landmarks sanity check : One of the landmarks is duplicate. You can try increasing the number of bins in the histogram \\\n",
    "                (NyulNormalizer.nbins) to avoid this behaviour. Landmarks are: ' + str(landmarks))\n",
    "\n",
    "        elif not (sorted(landmarks) == list(landmarks)):\n",
    "            Flag=False\n",
    "\n",
    "        return Flag\n",
    "            # raise Exception(\n",
    "            #     'ERROR NyulNormalizer landmarks sanity check: Landmarks in the list are not sorted, while they should be. Landmarks are: ' + str(\n",
    "            #         landmarks))\n",
    "def train(image_list,dir1,dir2,pLow=1, pHigh=99, sMin=1, sMax=99, numPoints=10,\n",
    "              showLandmarks=False,nbins=1024):\n",
    "\n",
    "        # Percentiles used to trunk the tails of the histogram\n",
    "        if pLow > 10:\n",
    "            raise (\"NyulNormalizer Error: pLow may be bigger than the first lm_pXX landmark.\")\n",
    "        if pHigh < 90:\n",
    "            raise (\"NyulNormalizer Error: pHigh may be bigger than the first lm_pXX landmark.\")\n",
    "\n",
    "        allMappedLandmarks = []\n",
    "\n",
    "        for image in image_list:\n",
    "\n",
    "            img = sitk.ReadImage(image)\n",
    "\n",
    "            landmarks = getLandmarks(img, showLandmarks=showLandmarks,nbins=nbins,pHigh=pHigh,pLow=pLow,numPoints=numPoints)\n",
    "                                    # Check the obtained landmarks ...\n",
    "            if landmarksSanityCheck(landmarks):\n",
    "                # Construct the linear mapping function\n",
    "                mapping = interp1d([landmarks[0], landmarks[-1]], [sMin, sMax], fill_value=(0,100))\n",
    "                # Map the landmarks to the standard scale\n",
    "                mappedLandmarks = mapping(landmarks)\n",
    "                # Add the mapped landmarks to the working set\n",
    "                allMappedLandmarks.append(mappedLandmarks)\n",
    "\n",
    "\n",
    "        meanLandmarks = np.array(allMappedLandmarks).mean(axis=0)\n",
    "            # Check the obtained landmarks ...\n",
    "        landmarksSanityCheck(meanLandmarks)\n",
    "        trainedModel = {\n",
    "                'pLow': pLow,\n",
    "                'pHigh': pHigh,\n",
    "                'sMin': sMin,\n",
    "                'sMax': sMax,\n",
    "                'numPoints': numPoints,\n",
    "                'meanLandmarks': meanLandmarks}\n",
    "\n",
    "        np.savez(dir2, trainedModel=[trainedModel])\n",
    "        return True\n",
    "def shif_by_negative_value(array):\n",
    "    array-=np.min(array)\n",
    "    return array\n",
    "def transform(image,meanLandmarks,mask=None):\n",
    "    # Get the raw data of the image\n",
    "    data = sitk.GetArrayFromImage(image)\n",
    "    # data = standardize(data, type='image')\n",
    "    # Calculate useful statistics\n",
    "    stats = sitk.StatisticsImageFilter()\n",
    "    stats.Execute(image)\n",
    "\n",
    "\n",
    "    # Get the landmarks for the current image\n",
    "    landmarks = getLandmarks(image, mask=mask, nbins=1024,pHigh=99,pLow=1,numPoints=10)\n",
    "    landmarks = np.array(landmarks)\n",
    "    # print(landmarks)\n",
    "    # Check the obtained landmarks ...\n",
    "    landmarksSanityCheck(landmarks)\n",
    "\n",
    "    # Recover the standard scale landmarks\n",
    "    standardScale = meanLandmarks\n",
    "\n",
    "\n",
    "    # Construct the piecewise linear interpolator to map the landmarks to the standard scale\n",
    "    mapping = interp1d(landmarks, standardScale, fill_value=\"extrapolate\")\n",
    "\n",
    "    # Map the input image to the standard space using the piecewise linear function\n",
    "\n",
    "    flatData = data.ravel()\n",
    "    tic()\n",
    "    mappedData = mapping(flatData)\n",
    "    mappedlandmarks = mapping(landmarks)\n",
    "    histo,bins=np.histogram(mappedData, 1024)\n",
    "    toc()\n",
    "    mappedData = mappedData.reshape(data.shape)\n",
    "\n",
    "    output = sitk.GetImageFromArray(shif_by_negative_value(mappedData.astype(int)))\n",
    "    output.SetSpacing(image.GetSpacing())\n",
    "    output.SetOrigin(image.GetOrigin())\n",
    "    output.SetDirection(image.GetDirection())\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################3\n",
    "\n",
    "\n",
    "trainedModelsBasicPath='/home/sliceruser/data/preprocess/standarizationModels'\n",
    "\n",
    "def trainStandarization(seriesString,train_patientsPaths):\n",
    "    \"\"\"\n",
    "    seriesString - marking with what we are deling like t2w ,adc etc\n",
    "    train_patientsPaths - list of paths to mha files we use to define standard image values\n",
    "    \"\"\"\n",
    "    trainedModel=join(trainedModelsBasicPath,'trained_model'+seriesString+'.npz')\n",
    "    train(train_patientsPaths, dir1=join(\"/home/sliceruser/data/preprocess/Bias_field_corrected\",seriesString),\n",
    "                                    dir2=trainedModel)\n",
    "    f = np.load(trainedModel, allow_pickle=True)\n",
    "    Model = f['trainedModel'].all()\n",
    "    meanLandmarks = Model['meanLandmarks']\n",
    "    return meanLandmarks       \n",
    "\n",
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "\n",
    "\n",
    "def removeOutliersAndWrite(path):\n",
    "    image=removeOutliersBiasFieldCorrect(path)\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(path)\n",
    "    writer.Execute(image)   \n",
    "\n",
    "def standardizeFromPathAndOverwrite(path,meanLandmarks):    \n",
    "    image=sitk.ReadImage(path)\n",
    "    image= transform(image,meanLandmarks=meanLandmarks)\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(path)\n",
    "    writer.Execute(image)    \n",
    "    \n",
    "def iterateAndStandardize(seriesString,numRows,df):\n",
    "    \"\"\"\n",
    "    iterates over files from train_patientsPaths representing seriesString type of the study\n",
    "    and overwrites it with normalised biased corrected and standardised version\n",
    "    numRows - marks how many rows we want to process\n",
    "    \"\"\"\n",
    "    #paralelize https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\n",
    "    train_patientsPaths=df[seriesString].dropna().astype('str')[(df[seriesString].str.len() >2)].to_numpy()[0:numRows]\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(removeOutliersAndWrite,train_patientsPaths)\n",
    "\n",
    "    toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "    toUp[0:numRows]=np.full(numRows, True)\n",
    "    colName= 'stand_and_bias_'+seriesString\n",
    "    df[colName]=toUp \n",
    "    \n",
    "    meanLandmarks=trainStandarization(seriesString,train_patientsPaths)\n",
    "\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(standardizeFromPathAndOverwrite,train_patientsPaths,meanLandmarks)\n",
    "\n",
    "\n",
    "    toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "    toUp[0:numRows]=np.full(numRows, True)\n",
    "    colName= 'Nyul_'+seriesString\n",
    "    df[colName]=toUp \n",
    "\n",
    "#Important !!! set all labels that are non 0 to 1\n",
    "def changeLabelToOnes(path):\n",
    "    \"\"\"\n",
    "    as in the labels or meaningfull ones are greater then 0 so we need to process it and change any nymber grater to 0 to 1...\n",
    "    \"\"\"\n",
    "    if(path!= \" \" and path!=\"\"):\n",
    "        image1 = sitk.ReadImage(path)\n",
    "        data = sitk.GetArrayFromImage(image1)\n",
    "        data -= np.min(data)\n",
    "        data[data>= 1] = 1\n",
    "        #recreating image keeping relevant metadata\n",
    "        image = sitk.GetImageFromArray(data)\n",
    "        image.SetSpacing(image1.GetSpacing())\n",
    "        image.SetOrigin(image1.GetOrigin())\n",
    "        image.SetDirection(image1.GetDirection())\n",
    "        writer = sitk.ImageFileWriter()\n",
    "        writer.KeepOriginalImageUIDOn()\n",
    "        writer.SetFileName(path)\n",
    "        writer.Execute(image)   \n",
    "    \n",
    "def iterateAndchangeLabelToOnes(numRows,df):\n",
    "    \"\"\"\n",
    "    iterates over files from train_patientsPaths representing seriesString type of the study\n",
    "    and overwrites it with normalised biased corrected and standardised version\n",
    "    \"\"\"\n",
    "    #paralelize https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\n",
    "    train_patientsPaths=df['reSampledPath'].dropna().astype('str')[(df['reSampledPath'].str.len() >2)].to_numpy()[0:numRows]\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(changeLabelToOnes,train_patientsPaths)\n",
    "    toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "    toUp[0:numRows]=np.full(numRows, True)\n",
    "    df['labels_to_one']=toUp    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "numRows=3\n",
    "iterateAndchangeLabelToOnes(numRows,df)\n",
    "for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]:\n",
    "    iterateAndStandardize(keyWord,numRows,df)\n",
    "\n",
    "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n",
    "\n",
    "# df['removeOutliersBiasFieldCorrect']=False\n",
    "# toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "# toUp[0:3]=[True,True,True]\n",
    "# df['removeOutliersBiasFieldCorrect']=toUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['removeOutliersBiasFieldCorrect'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
