{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ae831f-4c1c-466d-aa20-55fdd9f2fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    Orientationd,\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Spacingd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Resize,\n",
    "    Resized,\n",
    "    RandSpatialCropd,\n",
    "        AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    adaptor,\n",
    "\n",
    "    Invertd,\n",
    ")\n",
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "# corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "# inputImage = sitk.Cast(image, sitk.sitkFloat32) #required by N4BiasFieldCorrectionImageFilter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dffdebe-63b3-47e5-b8d1-de31c734bc4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'SyntaxError'>",
     "evalue": "invalid syntax (221489129.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [2]\u001b[0;36m\u001b[0m\n\u001b[0;31m    originalPath=metaDict[]\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def itk_biasField(x):\n",
    "    \"\"\"we need to take the metadata from original file plus modify on the basis of meta_dict\n",
    "    I assume only orientation and spacing may be changed\n",
    "    \"\"\"\n",
    "    print(x['t2w_meta_dict'])\n",
    "    #metaDict = x['t2w_meta_dict']\n",
    "    originalPath=metaDict[]\n",
    "    imageOrig = sitk.ReadImage(path)\n",
    "    imageMonai = sitk.GetImageFromArray(x['t2w'])\n",
    "    SetMetaData\n",
    "    #iterate over metadata of original file and \n",
    "    for key in image.GetMetaDataKeys():\n",
    "        print(\"\\\"{0}\\\":\\\"{1}\\\"\".format(key, image.GetMetaData(key)))\n",
    "    \n",
    "    # print(image.GetSize())\n",
    "    # print(image.GetOrigin())\n",
    "    # print(image.GetSpacing())\n",
    "    # print(image.GetDirection())\n",
    "    \n",
    "    \n",
    "    # smoothed = []\n",
    "    # for channel in x[\"image\"]:\n",
    "    #     smoothed.append(itk.median_image_filter(channel, radius=2))\n",
    "    # x[\"image\"] = np.stack(smoothed)\n",
    "    return x\n",
    "\n",
    "http://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/01_Image_Basics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f50dea-0623-487f-be19-5faccf120285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0008|0020': '2019-07-02',\n",
       " '0008|0060': 'MR',\n",
       " '0008|0070': 'SIEMENS',\n",
       " '0008|1090': 'Skyra',\n",
       " '0010|0020': '10000',\n",
       " '0010|0040': 'M',\n",
       " '0010|1010': '073Y',\n",
       " '0012|0062': 'YES',\n",
       " '0020|000d': '1000000',\n",
       " 'ANONYMISATION_SCRIPT': 'PI-CAI anonymisation script v2.0',\n",
       " 'PROSTATE_VOLUME_REPORT': '55',\n",
       " 'PSAD_REPORT': 'nan',\n",
       " 'PSA_REPORT': '7.7',\n",
       " 'spacing': array([0.28125   , 0.28125   , 3.29999998]),\n",
       " 'original_affine': array([[ -0.28125   ,   0.        ,   0.        , 109.22891617],\n",
       "        [  0.        ,  -0.2586998 ,  -1.29471247,  76.61426051],\n",
       "        [  0.        ,  -0.11034481,   3.03541093, -84.29311444],\n",
       "        [  0.        ,   0.        ,   0.        ,   1.        ]]),\n",
       " 'affine': array([[ 2.8125000e-01,  0.0000000e+00,  0.0000000e+00, -7.0489838e+01],\n",
       "        [ 0.0000000e+00,  2.5869980e-01, -1.2947124e+00, -8.8694908e+01],\n",
       "        [ 0.0000000e+00,  1.1034481e-01,  3.0354109e+00, -1.5480345e+02],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00]],\n",
       "       dtype=float32),\n",
       " 'spatial_shape': array([640, 640,  31]),\n",
       " 'original_channel_dim': 'no_channel',\n",
       " 'filename_or_obj': '/home/sliceruser/data/10000/10000_1000000_t2w.mha'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playgroundTrans = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"t2w\", \"label\"]),\n",
    "            EnsureChannelFirstd(keys=[\"t2w\", \"label\"]),\n",
    "            Orientationd(keys=[\"t2w\", \"label\"], axcodes=\"RAS\"),\n",
    "            EnsureTyped(keys=[\"t2w\", \"label\"],dtype=torch.float),\n",
    "            #itk_biasField,\n",
    "            # Spacingd(keys=[\"t2w\", \"label\"], pixdim=(\n",
    "            #     1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "            tio.transforms.EnsureShapeMultiple((32 , 32, 32), include=[\"t2w\", \"label\"]),\n",
    "            #CropForegroundd(keys=[\"t2w\", \"label\"], source_key=\"image\"),\n",
    "\n",
    "            # RandCropByPosNegLabeld(\n",
    "            #     keys=[\"t2w\", \"label\"],\n",
    "            #     label_key=\"label\",\n",
    "            #     spatial_size=(32, 32, 32),\n",
    "            #     pos=1,\n",
    "            #     neg=1,\n",
    "            #     num_samples=4,\n",
    "            #     image_key=\"t2w\",\n",
    "            #     image_threshold=0,\n",
    "            # ),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# {'0008|0020': '2019-07-02',\n",
    "#  '0008|0060': 'MR',\n",
    "#  '0008|0070': 'SIEMENS',\n",
    "#  '0008|1090': 'Skyra',\n",
    "#  '0010|0020': '10000',\n",
    "#  '0010|0040': 'M',\n",
    "#  '0010|1010': '073Y',\n",
    "#  '0012|0062': 'YES',\n",
    "#  '0020|000d': '1000000',\n",
    "#  'ANONYMISATION_SCRIPT': 'PI-CAI anonymisation script v2.0',\n",
    "#  'PROSTATE_VOLUME_REPORT': '55',\n",
    "#  'PSAD_REPORT': 'nan',\n",
    "#  'PSA_REPORT': '7.7',\n",
    "#  'spacing': array([0.28125   , 0.28125   , 3.29999998]),\n",
    "#  'original_affine': array([[ -0.28125   ,   0.        ,   0.        , 109.22891617],\n",
    "#         [  0.        ,  -0.2586998 ,  -1.29471247,  76.61426051],\n",
    "#         [  0.        ,  -0.11034481,   3.03541093, -84.29311444],\n",
    "#         [  0.        ,   0.        ,   0.        ,   1.        ]]),\n",
    "#  'affine': array([[ 2.8125000e-01,  0.0000000e+00,  0.0000000e+00, -7.0489838e+01],\n",
    "#         [ 0.0000000e+00,  2.5869980e-01, -1.2947124e+00, -8.8694908e+01],\n",
    "#         [ 0.0000000e+00,  1.1034481e-01,  3.0354109e+00, -1.5480345e+02],\n",
    "#         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00]],\n",
    "#        dtype=float32),\n",
    "#  'spatial_shape': array([640, 640,  31]),\n",
    "#  'original_channel_dim': 'no_channel',\n",
    "#  'filename_or_obj': '/home/sliceruser/data/10000/10000_1000000_t2w.mha'}\n",
    "\n",
    "\n",
    "testDict={\"t2w\":\"/home/sliceruser/data/10000/10000_1000000_t2w.mha\", \"label\": \"/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/10000_1000000.nii.gz\"}    \n",
    "playgroundTrans(testDict)['t2w_meta_dict']    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b1efb9-7777-48a5-982f-c4d7acc5d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathBaselineImage ='/home/sliceruser/data/10001/10001_1000001_t2w.mha'\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import collections\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "from scipy.stats import iqr\n",
    "\n",
    "# used to remove outlier values - when higher or lowe values will get clipped\n",
    "\n",
    "\n",
    "testDict={\"t2w\":\"/home/sliceruser/data/10000/10000_1000000_t2w.mha\", \"label\": \"/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/10000_1000000.nii.gz\"}    \n",
    "pathh=testDict['t2w']\n",
    "\n",
    "def removeOutliersBiasFieldCorrect(path,numberOfStandardDeviations = 4):\n",
    "    \"\"\"\n",
    "    all taken from https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/Main_Preprocessing.py\n",
    "    my modification that instead of histogram usage for outliers I use the standard deviations\n",
    "    path - path to file to be processed\n",
    "    numberOfStandardDeviations- osed to define outliers\n",
    "\n",
    "    \"\"\"\n",
    "    image1 = sitk.ReadImage(pathh)\n",
    "    data = sitk.GetArrayFromImage(image1)\n",
    "    # shift the data up so that all intensity values turn positive\n",
    "    stdd = np.std(data)*5\n",
    "    median = np.median(data)\n",
    "    data = np.clip(data, median-numberOfStandardDeviations*stdd, median+numberOfStandardDeviations*stdd)\n",
    "    data -= np.min(data)\n",
    "    #TO normalize an image by mapping its [Min,Max] into the interval [0,255]\n",
    "    N=255\n",
    "    data=N*(data+600)/2000\n",
    "\n",
    "    #recreating image keeping relevant metadata\n",
    "    image = sitk.GetImageFromArray(data)\n",
    "    image.SetSpacing(image1.GetSpacing())\n",
    "    image.SetOrigin(image1.GetOrigin())\n",
    "    image.SetDirection(image1.GetDirection())\n",
    "    #bias field normalization\n",
    "    maskImage = sitk.OtsuThreshold(image, 0, 1, 200)\n",
    "    inputImage = sitk.Cast(image, sitk.sitkFloat32)\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    # numberFittingLevels = 4\n",
    "    imageB = corrector.Execute(inputImage, maskImage)\n",
    "    imageB.SetSpacing(image.GetSpacing())\n",
    "    imageB.SetOrigin(image.GetOrigin())\n",
    "    imageB.SetDirection(image.GetDirection())\n",
    "    return imageB\n",
    "\n",
    "\n",
    "# imageOrig = sitk.ReadImage(pathh)\n",
    "# corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "# inputImage = sitk.Cast(image, sitk.sitkFloat32) #required by N4BiasFieldCorrectionImageFilter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e6924e-a251-4516-89b6-feecb2c2f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarization by https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/Main_Preprocessing.py\n",
    "imagee=removeOutliersBiasFieldCorrect(pathh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70dfacbc-6bbe-48a8-967e-6d7b2e1e2e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MRMLCore.vtkMRMLScalarVolumeNode(0x13715ae0) at 0x7f6cfe3ca040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import JupyterNotebooksLib as slicernb\n",
    "\n",
    "import sitkUtils\n",
    "import slicer\n",
    "\n",
    "outputVolumeNode = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLScalarVolumeNode\", \"MRHeadFiltered\")\n",
    "sitkUtils.PushVolumeToSlicer(imagee, outputVolumeNode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca5fd66e-c5ad-48e6-844f-05cce605509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MRMLCore.vtkMRMLScalarVolumeNode(0x13d34de0) at 0x7f6cfe3ca9a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1 = sitk.ReadImage(pathh)\n",
    "outputVolumeNode = slicer.mrmlScene.AddNewNodeByClass(\"vtkMRMLScalarVolumeNode\", \"original\")\n",
    "sitkUtils.PushVolumeToSlicer(image1, outputVolumeNode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "663cbcee-3dc7-4822-b687-65e10c0eb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no cor in 1000792\n",
      "no cor in 1000960\n",
      "no cor in 1001240\n",
      "no sag in 1000116\n",
      "no sag in 1000707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       /home/sliceruser/data/orig/10000/10000_1000000...\n",
       "1       /home/sliceruser/data/orig/10001/10001_1000001...\n",
       "2       /home/sliceruser/data/orig/10002/10002_1000002...\n",
       "3       /home/sliceruser/data/orig/10003/10003_1000003...\n",
       "4       /home/sliceruser/data/orig/10004/10004_1000004...\n",
       "                              ...                        \n",
       "1495    /home/sliceruser/data/orig/11471/11471_1001495...\n",
       "1496    /home/sliceruser/data/orig/11472/11472_1001496...\n",
       "1497    /home/sliceruser/data/orig/11473/11473_1001497...\n",
       "1498    /home/sliceruser/data/orig/11474/11474_1001498...\n",
       "1499    /home/sliceruser/data/orig/11475/11475_1001499...\n",
       "Name: t2w, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiprocessing python https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "csvPath='/home/sliceruser/labels/clinical_information/marksheet.csv'\n",
    "targetDir= '/home/sliceruser/data/orig'\n",
    "\n",
    "#create a dictionary of directories where key is the patient_id\n",
    "\n",
    "dirDict={}\n",
    "for subdir, dirs, files in os.walk(targetDir):\n",
    "    for subdirin, dirsin, filesin in os.walk(subdir):\n",
    "        lenn= len(filesin)\n",
    "        if(lenn>0):\n",
    "            dirDict[subdirin.split(\"/\")[5]]=filesin\n",
    "\n",
    "labelsFiles=[]\n",
    "labelsRootDir = '/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/'\n",
    "for subdir, dirs, files in os.walk(labelsRootDir):\n",
    "    labelsFiles=files\n",
    "    \n",
    "#Constructing functions that when applied to each row will fill the necessary path data\n",
    "listOfDeficientStudyIds=[]\n",
    "\n",
    "def findPathh(row,dirDictt,keyWord,targetDir):\n",
    "    patId=str(row['patient_id'])\n",
    "    study_id=str(row['study_id'])\n",
    "    #first check is such key present\n",
    "    if(patId in dirDictt ):\n",
    "        filtered = list(filter(lambda file_name:   (keyWord in file_name and  study_id  in  file_name  ), dirDictt[patId]  ))\n",
    "        if(len(filtered)>0):\n",
    "            return os.path.join(targetDir,  patId, filtered[0] )\n",
    "        else:\n",
    "            print(f\"no {keyWord} in {study_id}\")\n",
    "            listOfDeficientStudyIds.append(study_id)\n",
    "            return \" \" \n",
    "    listOfDeficientStudyIds.append(study_id)\n",
    "    return \" \"\n",
    "    \n",
    "\n",
    "def addPathsToDf(dff, dirDictt, keyWord):\n",
    "    return dff.apply(lambda row : findPathh(row,dirDictt ,keyWord,targetDir )   , axis = 1)\n",
    "\n",
    "df['t2w'] =addPathsToDf(df,dirDict, 't2w')\n",
    "df[\"adc\"] = addPathsToDf(df,dirDict, 'adc')\n",
    "df[\"cor\"] = addPathsToDf(df,dirDict, 'cor')\n",
    "df[\"hbv\"] = addPathsToDf(df,dirDict, 'hbv')\n",
    "df[\"sag\"] = addPathsToDf(df,dirDict, 'sag')\n",
    "#now  resampled labels are in separate directory\n",
    "\n",
    "\n",
    "def findResampledLabel(row,labelsFiles):\n",
    "    patId=str(row['patient_id'])\n",
    "    study_id=str(row['study_id'])\n",
    "    filtered = list(filter(lambda file_name:   (study_id  in  file_name  ), labelsFiles ))\n",
    "    if(len(filtered)>0):\n",
    "        return os.path.join(labelsRootDir, filtered[0])\n",
    "    listOfDeficientStudyIds.append(study_id)    \n",
    "    return \" \"\n",
    "    \n",
    "        \n",
    "df[\"reSampledPath\"] =  df.apply(lambda row : findResampledLabel(row,labelsFiles )   , axis = 1)  \n",
    "\n",
    "def isAnythingInAnnotated(row):\n",
    "    reSampledPath=str(row['reSampledPath'])\n",
    "    if(len(reSampledPath)>1):\n",
    "        image = sitk.ReadImage(reSampledPath)\n",
    "        nda = sitk.GetArrayFromImage(image)\n",
    "        return np.sum(nda)\n",
    "    return 0\n",
    "\n",
    "df[\"isAnythingInAnnotated\"]= df.apply(lambda row : isAnythingInAnnotated(row), axis = 1)  \n",
    "\n",
    "#marking that we have something lacking here\n",
    "df[\"isAnyMissing\"]=df.apply(lambda row : str(row['study_id']) in  listOfDeficientStudyIds  , axis = 1) \n",
    "\n",
    "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40588215-139f-4be6-8ae7-955a654e01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c4bf0-7d4c-41d1-8995-60190cf44cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0.1  Unnamed: 0  patient_id  study_id    mri_date  patient_age  \\\n",
      "0                0           0       10000   1000000  2019-07-02           73   \n",
      "1                1           1       10001   1000001  2016-05-27           64   \n",
      "2                2           2       10002   1000002  2021-04-18           58   \n",
      "3                3           3       10003   1000003  2019-04-05           72   \n",
      "4                4           4       10004   1000004  2020-10-21           67   \n",
      "...            ...         ...         ...       ...         ...          ...   \n",
      "1495          1495        1495       11471   1001495  2012-08-25           71   \n",
      "1496          1496        1496       11472   1001496  2019-06-28           81   \n",
      "1497          1497        1497       11473   1001497  2017-09-24           56   \n",
      "1498          1498        1498       11474   1001498  2016-05-03           71   \n",
      "1499          1499        1499       11475   1001499  2012-12-23           56   \n",
      "\n",
      "        psa  psad  prostate_volume histopath_type  ...   spac_t2W   spac_adc  \\\n",
      "0      7.70   NaN             55.0           MRBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "1      8.70  0.09            102.0            NaN  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "2      4.20  0.06             74.0            NaN  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "3     13.00   NaN             71.5          SysBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "4      8.00  0.10             78.0     SysBx+MRBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "...     ...   ...              ...            ...  ...        ...        ...   \n",
      "1495  12.50  0.21             62.0           MRBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "1496   5.28  0.12             44.0     SysBx+MRBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "1497  29.60  0.34             87.0           MRBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "1498  12.00   NaN             83.0           MRBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "1499  15.00  0.46             33.0           MRBx  ...  (0, 0, 0)  (0, 0, 0)   \n",
      "\n",
      "       spac_cor   spac_hbv   spac_sag   orig_t2W   orig_adc   orig_cor  \\\n",
      "0     (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "1     (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "2     (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "3     (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "4     (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1495  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "1496  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "1497  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "1498  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "1499  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)  (0, 0, 0)   \n",
      "\n",
      "       orig_hbv   orig_sag  \n",
      "0     (0, 0, 0)  (0, 0, 0)  \n",
      "1     (0, 0, 0)  (0, 0, 0)  \n",
      "2     (0, 0, 0)  (0, 0, 0)  \n",
      "3     (0, 0, 0)  (0, 0, 0)  \n",
      "4     (0, 0, 0)  (0, 0, 0)  \n",
      "...         ...        ...  \n",
      "1495  (0, 0, 0)  (0, 0, 0)  \n",
      "1496  (0, 0, 0)  (0, 0, 0)  \n",
      "1497  (0, 0, 0)  (0, 0, 0)  \n",
      "1498  (0, 0, 0)  (0, 0, 0)  \n",
      "1499  (0, 0, 0)  (0, 0, 0)  \n",
      "\n",
      "[1500 rows x 34 columns]\n",
      "train_patientsPaths\n",
      "['/home/sliceruser/data/orig/10000/10000_1000000_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10001/10001_1000001_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10002/10002_1000002_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10003/10003_1000003_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10004/10004_1000004_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10005/10005_1000005_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10006/10006_1000006_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10007/10007_1000007_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10008/10008_1000008_t2w.mha'\n",
      " '/home/sliceruser/data/orig/10009/10009_1000009_t2w.mha']\n"
     ]
    }
   ],
   "source": [
    "# pathBaselineImage ='/home/sliceruser/data/10001/10001_1000001_t2w.mha'\n",
    "from __future__ import print_function\n",
    "import SimpleITK as sitk\n",
    "from os import listdir\n",
    "from scipy.interpolate import interp1d\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import isdir,join,exists,split,dirname,basename\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def removeOutliersBiasFieldCorrect(path,numberOfStandardDeviations = 4):\n",
    "    \"\"\"\n",
    "    all taken from https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/Main_Preprocessing.py\n",
    "    my modification that instead of histogram usage for outliers I use the standard deviations\n",
    "    path - path to file to be processed\n",
    "    numberOfStandardDeviations- osed to define outliers\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    image1 = sitk.ReadImage(path)\n",
    "    data = sitk.GetArrayFromImage(image1)\n",
    "    # shift the data up so that all intensity values turn positive\n",
    "    stdd = np.std(data)*5\n",
    "    median = np.median(data)\n",
    "    data = np.clip(data, median-numberOfStandardDeviations*stdd, median+numberOfStandardDeviations*stdd)\n",
    "    data -= np.min(data)\n",
    "    #TO normalize an image by mapping its [Min,Max] into the interval [0,255]\n",
    "    N=255\n",
    "    data=N*(data+600)/2000\n",
    "\n",
    "    #recreating image keeping relevant metadata\n",
    "    image = sitk.GetImageFromArray(data)\n",
    "    image.SetSpacing(image1.GetSpacing())\n",
    "    image.SetOrigin(image1.GetOrigin())\n",
    "    image.SetDirection(image1.GetDirection())\n",
    "    #bias field normalization\n",
    "    maskImage = sitk.OtsuThreshold(image, 0, 1, 200)\n",
    "    inputImage = sitk.Cast(image, sitk.sitkFloat32)\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    # numberFittingLevels = 4\n",
    "    imageB = corrector.Execute(inputImage, maskImage)\n",
    "    imageB.SetSpacing(image.GetSpacing())\n",
    "    imageB.SetOrigin(image.GetOrigin())\n",
    "    imageB.SetDirection(image.GetDirection())\n",
    "    return imageB\n",
    "\n",
    "#####################\n",
    "#taken and adapted from https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/image/Nyul_preprocessing.py\n",
    "# !/usr/bin/env python3\n",
    "# -------------------------------------------------------------------------------\n",
    "# Author: Samira Masoudi\n",
    "# Date:   11.07.2019\n",
    "# Based on Nyul et al. 2000: New variants of a method of MRI scale standardization\n",
    "# Full version can be found at https://github.com/sergivalverde/MRI_intensity_normalization\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "def tic():\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "\n",
    "def toc():\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        print\n",
    "        \"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\"\n",
    "    else:\n",
    "        print\n",
    "        \"Toc: start time not set\"\n",
    "\n",
    "\n",
    "def getCdf(hist):\n",
    "    \"\"\"\n",
    "        Given a histogram, it returns the cumulative distribution function.\n",
    "    \"\"\"\n",
    "    aux = np.cumsum(hist)\n",
    "    aux = aux / aux[-1] * 100\n",
    "    return aux\n",
    "\n",
    "\n",
    "def getPercentile(cdf, bins, perc):\n",
    "    \"\"\"\n",
    "        Given a cumulative distribution function obtained from a histogram,\n",
    "        (where 'bins' are the x values of the histogram and 'cdf' is the\n",
    "        cumulative distribution function of the original histogram), it returns\n",
    "        the x center value for the bin index corresponding to the given percentile,\n",
    "        and the bin index itself.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            import numpy as np\n",
    "            hist = np.array([204., 1651., 2405., 1972., 872., 1455.])\n",
    "            bins = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "\n",
    "            cumHist = getCdf(hist)\n",
    "            print cumHist\n",
    "            val, bin = getPercentile(cumHist, bins, 50)\n",
    "\n",
    "            print \"Val = \" + str(val)\n",
    "            print \"Bin = \" + str(bin)\n",
    "\n",
    "    \"\"\"\n",
    "    b = len(bins[cdf <= perc])\n",
    "    return bins[b] + ((bins[1] - bins[0]) / 2)\n",
    "\n",
    "\n",
    "\n",
    "def getLandmarks(image, mask=None, showLandmarks=False,nbins=1024, pLow=1, pHigh=99,numPoints=10):\n",
    "        \"\"\"\n",
    "            This Private function obtain the landmarks for a given image and returns them\n",
    "            in a list like:\n",
    "                [lm_pLow, lm_perc1, lm_perc2, ... lm_perc_(numPoints-1), lm_pHigh] (lm means landmark)\n",
    "\n",
    "            :param image    SimpleITK image for which the landmarks are computed.\n",
    "            :param mask     [OPTIONAL] SimpleITK image containing a mask. If provided, the histogram will be computed\n",
    "                                    taking into account only the voxels where mask > 0.\n",
    "            :param showLandmarks    Plot the landmarks using matplotlib on top of the histogram.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data = sitk.GetArrayFromImage(image)\n",
    "        if mask is None:\n",
    "            # Calculate useful statistics\n",
    "            stats = sitk.StatisticsImageFilter()\n",
    "            stats.Execute(image)\n",
    "            mean =stats.GetMean()\n",
    "\n",
    "            # Compute the image histogram\n",
    "            histo, bins = np.histogram(data.flatten(), nbins)\n",
    "\n",
    "            # Calculate the cumulative distribution function of the original histogram\n",
    "            cdfOriginal = getCdf(histo)\n",
    "\n",
    "            # Truncate the histogram (put 0 to those values whose intensity is less than the mean)\n",
    "            # so that only the foreground values are considered for the landmark learning process\n",
    "            histo[bins[:-1] < mean] = 0.0\n",
    "        # else:\n",
    "        #     # Calculate useful statistics\n",
    "        #     dataMask = sitk.GetArrayFromImage(mask)\n",
    "        #\n",
    "        #     # Compute the image histogram\n",
    "        #     histo, bins = np.histogram(data[dataMask > 0].flatten(), nbins, normed=True)\n",
    "        #\n",
    "        #     # Calculate the cumulative distribution function of the original histogram\n",
    "        #     cdfOriginal = getCdf(histo)\n",
    "\n",
    "        # Calculate the cumulative distribution function of the truncated histogram, where outliers are removed\n",
    "        cdfTruncated = getCdf(histo)\n",
    "\n",
    "        # Generate the percentile landmarks for  m_i\n",
    "        perc = [x for x in range(0, 100, 100 // numPoints)]\n",
    "        # Remove the first landmark that will always correspond to 0\n",
    "        perc = perc[1:]\n",
    "\n",
    "        # Generate the landmarks. Note that those corresponding to pLow and pHigh (at the beginning and the\n",
    "        # end of the list of landmarks) are generated from the cdfOriginal, while the ones\n",
    "        # corresponding to the percentiles are generated from cdfTruncated, meaning that only foreground intensities\n",
    "        # are considered.\n",
    "\n",
    "        landmarks = [getPercentile(cdfOriginal, bins[:-1], pLow)] + [getPercentile(cdfTruncated, bins[:-1], x) for x in perc] + [getPercentile(cdfOriginal, bins[:-1], pHigh)]\n",
    "        # landmarks_org =  [getPercentile(cdfOriginal, bins[:-1], x) for x in [pLow]+perc+[pHigh]]\n",
    "        return landmarks\n",
    "\n",
    "def  landmarksSanityCheck(landmarks):\n",
    "        Flag=True\n",
    "        if not (np.unique(landmarks).size == len(landmarks)):\n",
    "            for i in range(1, len(landmarks) - 1):\n",
    "                if landmarks[i] == landmarks[i + 1]:\n",
    "                    landmarks[i] = (landmarks[i - 1] + landmarks[i + 1]) / 2.0\n",
    "\n",
    "                print( \"WARNING: Fixing duplicate landmark.\")\n",
    "\n",
    "            if not (np.unique(landmarks).size == len(landmarks)):\n",
    "                raise Exception('ERROR NyulNormalizer landmarks sanity check : One of the landmarks is duplicate. You can try increasing the number of bins in the histogram \\\n",
    "                (NyulNormalizer.nbins) to avoid this behaviour. Landmarks are: ' + str(landmarks))\n",
    "\n",
    "        elif not (sorted(landmarks) == list(landmarks)):\n",
    "            Flag=False\n",
    "\n",
    "        return Flag\n",
    "            # raise Exception(\n",
    "            #     'ERROR NyulNormalizer landmarks sanity check: Landmarks in the list are not sorted, while they should be. Landmarks are: ' + str(\n",
    "            #         landmarks))\n",
    "def train(image_list,dir1,dir2,pLow=1, pHigh=99, sMin=1, sMax=99, numPoints=10,\n",
    "              showLandmarks=False,nbins=1024):\n",
    "\n",
    "        # Percentiles used to trunk the tails of the histogram\n",
    "        if pLow > 10:\n",
    "            raise (\"NyulNormalizer Error: pLow may be bigger than the first lm_pXX landmark.\")\n",
    "        if pHigh < 90:\n",
    "            raise (\"NyulNormalizer Error: pHigh may be bigger than the first lm_pXX landmark.\")\n",
    "\n",
    "        allMappedLandmarks = []\n",
    "\n",
    "        for image in image_list:\n",
    "\n",
    "            img = sitk.ReadImage(image)\n",
    "\n",
    "            landmarks = getLandmarks(img, showLandmarks=showLandmarks,nbins=nbins,pHigh=pHigh,pLow=pLow,numPoints=numPoints)\n",
    "                                    # Check the obtained landmarks ...\n",
    "            if landmarksSanityCheck(landmarks):\n",
    "                # Construct the linear mapping function\n",
    "                mapping = interp1d([landmarks[0], landmarks[-1]], [sMin, sMax], fill_value=(0,100))\n",
    "                # Map the landmarks to the standard scale\n",
    "                mappedLandmarks = mapping(landmarks)\n",
    "                # Add the mapped landmarks to the working set\n",
    "                allMappedLandmarks.append(mappedLandmarks)\n",
    "\n",
    "\n",
    "        meanLandmarks = np.array(allMappedLandmarks).mean(axis=0)\n",
    "            # Check the obtained landmarks ...\n",
    "        landmarksSanityCheck(meanLandmarks)\n",
    "        trainedModel = {\n",
    "                'pLow': pLow,\n",
    "                'pHigh': pHigh,\n",
    "                'sMin': sMin,\n",
    "                'sMax': sMax,\n",
    "                'numPoints': numPoints,\n",
    "                'meanLandmarks': meanLandmarks}\n",
    "\n",
    "        np.savez(dir2, trainedModel=[trainedModel])\n",
    "        return True\n",
    "def shif_by_negative_value(array):\n",
    "    array-=np.min(array)\n",
    "    return array\n",
    "def transform(image,meanLandmarks,mask=None):\n",
    "    # Get the raw data of the image\n",
    "    data = sitk.GetArrayFromImage(image)\n",
    "    # data = standardize(data, type='image')\n",
    "    # Calculate useful statistics\n",
    "    stats = sitk.StatisticsImageFilter()\n",
    "    stats.Execute(image)\n",
    "\n",
    "\n",
    "    # Get the landmarks for the current image\n",
    "    landmarks = getLandmarks(image, mask=mask, nbins=1024,pHigh=99,pLow=1,numPoints=10)\n",
    "    landmarks = np.array(landmarks)\n",
    "    # print(landmarks)\n",
    "    # Check the obtained landmarks ...\n",
    "    landmarksSanityCheck(landmarks)\n",
    "\n",
    "    # Recover the standard scale landmarks\n",
    "    standardScale = meanLandmarks\n",
    "\n",
    "\n",
    "    # Construct the piecewise linear interpolator to map the landmarks to the standard scale\n",
    "    mapping = interp1d(landmarks, standardScale, fill_value=\"extrapolate\")\n",
    "\n",
    "    # Map the input image to the standard space using the piecewise linear function\n",
    "\n",
    "    flatData = data.ravel()\n",
    "    tic()\n",
    "    mappedData = mapping(flatData)\n",
    "    mappedlandmarks = mapping(landmarks)\n",
    "    histo,bins=np.histogram(mappedData, 1024)\n",
    "    toc()\n",
    "    mappedData = mappedData.reshape(data.shape)\n",
    "\n",
    "    output = sitk.GetImageFromArray(shif_by_negative_value(mappedData.astype(int)))\n",
    "    output.SetSpacing(image.GetSpacing())\n",
    "    output.SetOrigin(image.GetOrigin())\n",
    "    output.SetDirection(image.GetDirection())\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################3\n",
    "\n",
    "\n",
    "trainedModelsBasicPath='/home/sliceruser/data/preprocess/standarizationModels'\n",
    "\n",
    "def trainStandarization(seriesString,train_patientsPaths):\n",
    "    \"\"\"\n",
    "    seriesString - marking with what we are deling like t2w ,adc etc\n",
    "    train_patientsPaths - list of paths to mha files we use to define standard image values\n",
    "    \"\"\"\n",
    "    trainedModel=join(trainedModelsBasicPath,'trained_model'+seriesString+'.npz')\n",
    "    train(train_patientsPaths, dir1=join(\"/home/sliceruser/data/preprocess/Bias_field_corrected\",seriesString),\n",
    "                                    dir2=trainedModel)\n",
    "    f = np.load(trainedModel, allow_pickle=True)\n",
    "    Model = f['trainedModel'].all()\n",
    "    meanLandmarks = Model['meanLandmarks']\n",
    "    return meanLandmarks       \n",
    "\n",
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "def removeOutliersAndWrite(path):\n",
    "    image=removeOutliersBiasFieldCorrect(path)\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(path)\n",
    "    writer.Execute(image)   \n",
    "\n",
    "def standardizeFromPathAndOverwrite(path,meanLandmarks):    \n",
    "    image=sitk.ReadImage(path)\n",
    "    image= transform(image,meanLandmarks=meanLandmarks)\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(path)\n",
    "    writer.Execute(image)    \n",
    "    \n",
    "def iterateAndStandardize(seriesString):\n",
    "    \"\"\"\n",
    "    iterates over files from train_patientsPaths representing seriesString type of the study\n",
    "    and overwrites it with normalised biased corrected and standardised version\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "    #paralelize https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\n",
    "    train_patientsPaths=df[seriesString].dropna().astype('str')[(df[seriesString].str.len() >2)].to_numpy()[0:10]\n",
    "    print(\"train_patientsPaths\")\n",
    "    print(train_patientsPaths)\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(removeOutliersAndWrite,train_patientsPaths)\n",
    "\n",
    "    \n",
    "    meanLandmarks=trainStandarization(seriesString,train_patientsPaths)\n",
    "\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(standardizeFromPathAndOverwrite,train_patientsPaths,meanLandmarks)\n",
    "\n",
    "iterateAndStandardize('t2w')\n",
    "iterateAndStandardize('adc')\n",
    "iterateAndStandardize('cor')\n",
    "iterateAndStandardize('hbv')\n",
    "iterateAndStandardize('sag')\n",
    "\n",
    "#Important !!! set all labels that are non 0 to 1\n",
    "def changeLabelToOnes(path):\n",
    "    \"\"\"\n",
    "    as in the labels or meaningfull ones are greater then 0 so we need to process it and change any nymber grater to 0 to 1...\n",
    "    \"\"\"\n",
    "    if(path!= \" \" and path!=\"\"):\n",
    "        image1 = sitk.ReadImage(path)\n",
    "        data = sitk.GetArrayFromImage(image1)\n",
    "        data -= np.min(data)\n",
    "        data[data>= 1] = 1\n",
    "        #recreating image keeping relevant metadata\n",
    "        image = sitk.GetImageFromArray(data)\n",
    "        image.SetSpacing(image1.GetSpacing())\n",
    "        image.SetOrigin(image1.GetOrigin())\n",
    "        image.SetDirection(image1.GetDirection())\n",
    "        writer = sitk.ImageFileWriter()\n",
    "        writer.KeepOriginalImageUIDOn()\n",
    "        writer.SetFileName(path)\n",
    "        writer.Execute(image)   \n",
    "    \n",
    "def iterateAndchangeLabelToOnes():\n",
    "    \"\"\"\n",
    "    iterates over files from train_patientsPaths representing seriesString type of the study\n",
    "    and overwrites it with normalised biased corrected and standardised version\n",
    "    \"\"\"\n",
    "    #paralelize https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\n",
    "    train_patientsPaths=df['reSampledPath'].dropna().astype('str')[(df['reSampledPath'].str.len() >2)].to_numpy()\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(changeLabelToOnes,train_patientsPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "057a5028-8710-4d2e-a56c-b37e5b57113e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/10000_1000000.nii.gz',\n",
       "       '/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/10001_1000001.nii.gz',\n",
       "       '/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/10002_1000002.nii.gz',\n",
       "       ...,\n",
       "       '/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/11473_1001497.nii.gz',\n",
       "       '/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/11474_1001498.nii.gz',\n",
       "       '/home/sliceruser/labels/csPCa_lesion_delineations/human_expert/resampled/11475_1001499.nii.gz'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pathBaselineImage ='/home/sliceruser/data/10001/10001_1000001_t2w.mha'\n",
    "from __future__ import print_function\n",
    "import SimpleITK as sitk\n",
    "from os import listdir\n",
    "from scipy.interpolate import interp1d\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import isdir,join,exists,split,dirname,basename\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import collections\n",
    "import numpy as np\n",
    "seriesString='t2w'\n",
    "df['reSampledPath'].dropna().astype('str')[(df['reSampledPath'].str.len() >2)].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c225ae-324d-4da4-bd43-ce4fdc9beaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Slicer 5.0",
   "language": "python",
   "name": "slicer-5.0"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
