{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Standardize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/media/jakub/New Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000000?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000000?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000000?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mStandardize\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Standardize'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "from zipfile import ZipFile\n",
    "from zipfile import BadZipFile\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from functools import partial\n",
    "import Standardize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## managment of files \n",
    "managment of files is done via managePicaiFiles.sh - create directories download and unpack files saves basic metadata and do simple metadata preprocessing\n",
    "sh managePicaiFiles.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization\n",
    "primary preprocessing - removing ouliers, put values between 0 and 255 bias field correction Nyul standarization binarizing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathBaselineImage ='/home/sliceruser/data/10001/10001_1000001_t2w.mha'\n",
    "from __future__ import print_function\n",
    "import SimpleITK as sitk\n",
    "from os import listdir\n",
    "from scipy.interpolate import interp1d\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import isdir,join,exists,split,dirname,basename\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def removeOutliersBiasFieldCorrect(path,numberOfStandardDeviations = 4):\n",
    "    \"\"\"\n",
    "    all taken from https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/Main_Preprocessing.py\n",
    "    my modification that instead of histogram usage for outliers I use the standard deviations\n",
    "    path - path to file to be processed\n",
    "    numberOfStandardDeviations- osed to define outliers\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    image1 = sitk.ReadImage(path)\n",
    "    data = sitk.GetArrayFromImage(image1)\n",
    "    # shift the data up so that all intensity values turn positive\n",
    "    stdd = np.std(data)*5\n",
    "    median = np.median(data)\n",
    "    data = np.clip(data, median-numberOfStandardDeviations*stdd, median+numberOfStandardDeviations*stdd)\n",
    "    data -= np.min(data)\n",
    "    #TO normalize an image by mapping its [Min,Max] into the interval [0,255]\n",
    "    N=255\n",
    "    data=N*(data+600)/2000\n",
    "\n",
    "    #recreating image keeping relevant metadata\n",
    "    image = sitk.GetImageFromArray(data)\n",
    "    image.SetSpacing(image1.GetSpacing())\n",
    "    image.SetOrigin(image1.GetOrigin())\n",
    "    image.SetDirection(image1.GetDirection())\n",
    "    #bias field normalization\n",
    "    maskImage = sitk.OtsuThreshold(image, 0, 1, 200)\n",
    "    inputImage = sitk.Cast(image, sitk.sitkFloat32)\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    # numberFittingLevels = 4\n",
    "    imageB = corrector.Execute(inputImage, maskImage)\n",
    "    imageB.SetSpacing(image.GetSpacing())\n",
    "    imageB.SetOrigin(image.GetOrigin())\n",
    "    imageB.SetDirection(image.GetDirection())\n",
    "    return imageB\n",
    "\n",
    "#####################\n",
    "#taken and adapted from https://github.com/NIH-MIP/Radiology_Image_Preprocessing_for_Deep_Learning/blob/main/Codes/image/Nyul_preprocessing.py\n",
    "# !/usr/bin/env python3\n",
    "# -------------------------------------------------------------------------------\n",
    "# Author: Samira Masoudi\n",
    "# Date:   11.07.2019\n",
    "# Based on Nyul et al. 2000: New variants of a method of MRI scale standardization\n",
    "# Full version can be found at https://github.com/sergivalverde/MRI_intensity_normalization\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "def tic():\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "\n",
    "def toc():\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        print\n",
    "        \"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\"\n",
    "    else:\n",
    "        print\n",
    "        \"Toc: start time not set\"\n",
    "\n",
    "\n",
    "def getCdf(hist):\n",
    "    \"\"\"\n",
    "        Given a histogram, it returns the cumulative distribution function.\n",
    "    \"\"\"\n",
    "    aux = np.cumsum(hist)\n",
    "    aux = aux / aux[-1] * 100\n",
    "    return aux\n",
    "\n",
    "\n",
    "def getPercentile(cdf, bins, perc):\n",
    "    \"\"\"\n",
    "        Given a cumulative distribution function obtained from a histogram,\n",
    "        (where 'bins' are the x values of the histogram and 'cdf' is the\n",
    "        cumulative distribution function of the original histogram), it returns\n",
    "        the x center value for the bin index corresponding to the given percentile,\n",
    "        and the bin index itself.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            import numpy as np\n",
    "            hist = np.array([204., 1651., 2405., 1972., 872., 1455.])\n",
    "            bins = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "\n",
    "            cumHist = getCdf(hist)\n",
    "            print cumHist\n",
    "            val, bin = getPercentile(cumHist, bins, 50)\n",
    "\n",
    "            print \"Val = \" + str(val)\n",
    "            print \"Bin = \" + str(bin)\n",
    "\n",
    "    \"\"\"\n",
    "    b = len(bins[cdf <= perc])\n",
    "    return bins[b] + ((bins[1] - bins[0]) / 2)\n",
    "\n",
    "\n",
    "\n",
    "def getLandmarks(image, mask=None, showLandmarks=False,nbins=1024, pLow=1, pHigh=99,numPoints=10):\n",
    "        \"\"\"\n",
    "            This Private function obtain the landmarks for a given image and returns them\n",
    "            in a list like:\n",
    "                [lm_pLow, lm_perc1, lm_perc2, ... lm_perc_(numPoints-1), lm_pHigh] (lm means landmark)\n",
    "\n",
    "            :param image    SimpleITK image for which the landmarks are computed.\n",
    "            :param mask     [OPTIONAL] SimpleITK image containing a mask. If provided, the histogram will be computed\n",
    "                                    taking into account only the voxels where mask > 0.\n",
    "            :param showLandmarks    Plot the landmarks using matplotlib on top of the histogram.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data = sitk.GetArrayFromImage(image)\n",
    "        if mask is None:\n",
    "            # Calculate useful statistics\n",
    "            stats = sitk.StatisticsImageFilter()\n",
    "            stats.Execute(image)\n",
    "            mean =stats.GetMean()\n",
    "\n",
    "            # Compute the image histogram\n",
    "            histo, bins = np.histogram(data.flatten(), nbins)\n",
    "\n",
    "            # Calculate the cumulative distribution function of the original histogram\n",
    "            cdfOriginal = getCdf(histo)\n",
    "\n",
    "            # Truncate the histogram (put 0 to those values whose intensity is less than the mean)\n",
    "            # so that only the foreground values are considered for the landmark learning process\n",
    "            histo[bins[:-1] < mean] = 0.0\n",
    "        # else:\n",
    "        #     # Calculate useful statistics\n",
    "        #     dataMask = sitk.GetArrayFromImage(mask)\n",
    "        #\n",
    "        #     # Compute the image histogram\n",
    "        #     histo, bins = np.histogram(data[dataMask > 0].flatten(), nbins, normed=True)\n",
    "        #\n",
    "        #     # Calculate the cumulative distribution function of the original histogram\n",
    "        #     cdfOriginal = getCdf(histo)\n",
    "\n",
    "        # Calculate the cumulative distribution function of the truncated histogram, where outliers are removed\n",
    "        cdfTruncated = getCdf(histo)\n",
    "\n",
    "        # Generate the percentile landmarks for  m_i\n",
    "        perc = [x for x in range(0, 100, 100 // numPoints)]\n",
    "        # Remove the first landmark that will always correspond to 0\n",
    "        perc = perc[1:]\n",
    "\n",
    "        # Generate the landmarks. Note that those corresponding to pLow and pHigh (at the beginning and the\n",
    "        # end of the list of landmarks) are generated from the cdfOriginal, while the ones\n",
    "        # corresponding to the percentiles are generated from cdfTruncated, meaning that only foreground intensities\n",
    "        # are considered.\n",
    "\n",
    "        landmarks = [getPercentile(cdfOriginal, bins[:-1], pLow)] + [getPercentile(cdfTruncated, bins[:-1], x) for x in perc] + [getPercentile(cdfOriginal, bins[:-1], pHigh)]\n",
    "        # landmarks_org =  [getPercentile(cdfOriginal, bins[:-1], x) for x in [pLow]+perc+[pHigh]]\n",
    "        return landmarks\n",
    "\n",
    "def  landmarksSanityCheck(landmarks):\n",
    "        Flag=True\n",
    "        if not (np.unique(landmarks).size == len(landmarks)):\n",
    "            for i in range(1, len(landmarks) - 1):\n",
    "                if landmarks[i] == landmarks[i + 1]:\n",
    "                    landmarks[i] = (landmarks[i - 1] + landmarks[i + 1]) / 2.0\n",
    "\n",
    "                print( \"WARNING: Fixing duplicate landmark.\")\n",
    "\n",
    "            if not (np.unique(landmarks).size == len(landmarks)):\n",
    "                raise Exception('ERROR NyulNormalizer landmarks sanity check : One of the landmarks is duplicate. You can try increasing the number of bins in the histogram \\\n",
    "                (NyulNormalizer.nbins) to avoid this behaviour. Landmarks are: ' + str(landmarks))\n",
    "\n",
    "        elif not (sorted(landmarks) == list(landmarks)):\n",
    "            Flag=False\n",
    "\n",
    "        return Flag\n",
    "            # raise Exception(\n",
    "            #     'ERROR NyulNormalizer landmarks sanity check: Landmarks in the list are not sorted, while they should be. Landmarks are: ' + str(\n",
    "            #         landmarks))\n",
    "def train(image_list,dir1,dir2,pLow=1, pHigh=99, sMin=1, sMax=99, numPoints=10,\n",
    "              showLandmarks=False,nbins=1024):\n",
    "\n",
    "        # Percentiles used to trunk the tails of the histogram\n",
    "        if pLow > 10:\n",
    "            raise (\"NyulNormalizer Error: pLow may be bigger than the first lm_pXX landmark.\")\n",
    "        if pHigh < 90:\n",
    "            raise (\"NyulNormalizer Error: pHigh may be bigger than the first lm_pXX landmark.\")\n",
    "\n",
    "        allMappedLandmarks = []\n",
    "\n",
    "        for image in image_list:\n",
    "\n",
    "            img = sitk.ReadImage(image)\n",
    "\n",
    "            landmarks = getLandmarks(img, showLandmarks=showLandmarks,nbins=nbins,pHigh=pHigh,pLow=pLow,numPoints=numPoints)\n",
    "                                    # Check the obtained landmarks ...\n",
    "            if landmarksSanityCheck(landmarks):\n",
    "                # Construct the linear mapping function\n",
    "                mapping = interp1d([landmarks[0], landmarks[-1]], [sMin, sMax], fill_value=(0,100))\n",
    "                # Map the landmarks to the standard scale\n",
    "                mappedLandmarks = mapping(landmarks)\n",
    "                # Add the mapped landmarks to the working set\n",
    "                allMappedLandmarks.append(mappedLandmarks)\n",
    "\n",
    "\n",
    "        meanLandmarks = np.array(allMappedLandmarks).mean(axis=0)\n",
    "            # Check the obtained landmarks ...\n",
    "        landmarksSanityCheck(meanLandmarks)\n",
    "        trainedModel = {\n",
    "                'pLow': pLow,\n",
    "                'pHigh': pHigh,\n",
    "                'sMin': sMin,\n",
    "                'sMax': sMax,\n",
    "                'numPoints': numPoints,\n",
    "                'meanLandmarks': meanLandmarks}\n",
    "\n",
    "        np.savez(dir2, trainedModel=[trainedModel])\n",
    "        return True\n",
    "def shif_by_negative_value(array):\n",
    "    array-=np.min(array)\n",
    "    return array\n",
    "def transform(image,meanLandmarks,mask=None):\n",
    "    # Get the raw data of the image\n",
    "    data = sitk.GetArrayFromImage(image)\n",
    "    # data = standardize(data, type='image')\n",
    "    # Calculate useful statistics\n",
    "    stats = sitk.StatisticsImageFilter()\n",
    "    stats.Execute(image)\n",
    "\n",
    "\n",
    "    # Get the landmarks for the current image\n",
    "    landmarks = getLandmarks(image, mask=mask, nbins=1024,pHigh=99,pLow=1,numPoints=10)\n",
    "    landmarks = np.array(landmarks)\n",
    "    # print(landmarks)\n",
    "    # Check the obtained landmarks ...\n",
    "    landmarksSanityCheck(landmarks)\n",
    "\n",
    "    # Recover the standard scale landmarks\n",
    "    standardScale = meanLandmarks\n",
    "\n",
    "\n",
    "    # Construct the piecewise linear interpolator to map the landmarks to the standard scale\n",
    "    mapping = interp1d(landmarks, standardScale, fill_value=\"extrapolate\")\n",
    "\n",
    "    # Map the input image to the standard space using the piecewise linear function\n",
    "\n",
    "    flatData = data.ravel()\n",
    "    tic()\n",
    "    mappedData = mapping(flatData)\n",
    "    mappedlandmarks = mapping(landmarks)\n",
    "    histo,bins=np.histogram(mappedData, 1024)\n",
    "    toc()\n",
    "    mappedData = mappedData.reshape(data.shape)\n",
    "\n",
    "    output = sitk.GetImageFromArray(shif_by_negative_value(mappedData.astype(int)))\n",
    "    output.SetSpacing(image.GetSpacing())\n",
    "    output.SetOrigin(image.GetOrigin())\n",
    "    output.SetDirection(image.GetDirection())\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################3\n",
    "\n",
    "\n",
    "trainedModelsBasicPath='/home/sliceruser/data/preprocess/standarizationModels'\n",
    "\n",
    "def trainStandarization(seriesString,train_patientsPaths):\n",
    "    \"\"\"\n",
    "    seriesString - marking with what we are deling like t2w ,adc etc\n",
    "    train_patientsPaths - list of paths to mha files we use to define standard image values\n",
    "    \"\"\"\n",
    "    trainedModel=join(trainedModelsBasicPath,'trained_model'+seriesString+'.npz')\n",
    "    train(train_patientsPaths, dir1=join(\"/home/sliceruser/data/preprocess/Bias_field_corrected\",seriesString),\n",
    "                                    dir2=trainedModel)\n",
    "    f = np.load(trainedModel, allow_pickle=True)\n",
    "    Model = f['trainedModel'].all()\n",
    "    meanLandmarks = Model['meanLandmarks']\n",
    "    return meanLandmarks       \n",
    "\n",
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "\n",
    "\n",
    "def removeOutliersAndWrite(path):\n",
    "    #print(path)\n",
    "    image=removeOutliersBiasFieldCorrect(path)\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(path)\n",
    "    writer.Execute(image)\n",
    "    return 0   \n",
    "\n",
    "def standardizeFromPathAndOverwrite(path,meanLandmarks):\n",
    "    print(path)\n",
    "    image=sitk.ReadImage(path)\n",
    "    image= transform(image,meanLandmarks=meanLandmarks)\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    writer.KeepOriginalImageUIDOn()\n",
    "    writer.SetFileName(path)\n",
    "    writer.Execute(image)\n",
    "    return 0       \n",
    "    \n",
    "def iterateAndStandardize(seriesString,numRows,df):\n",
    "    \"\"\"\n",
    "    iterates over files from train_patientsPaths representing seriesString type of the study\n",
    "    and overwrites it with normalised biased corrected and standardised version\n",
    "    numRows - marks how many rows we want to process\n",
    "    \"\"\"\n",
    "    #paralelize https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\n",
    "    train_patientsPaths=df[seriesString].dropna().astype('str')[(df[seriesString].str.len() >2)].to_numpy()[0:numRows]\n",
    "    #print(train_patientsPaths)\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(removeOutliersAndWrite,train_patientsPaths)\n",
    "    numRows=len(train_patientsPaths)\n",
    "    toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "    toUp[0:numRows]=np.full(numRows, True)\n",
    "    colName= 'stand_and_bias_'+seriesString\n",
    "    df[colName]=toUp \n",
    "    \n",
    "    meanLandmarks=trainStandarization(seriesString,train_patientsPaths)\n",
    "    print(meanLandmarks)\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(partial(standardizeFromPathAndOverwrite,meanLandmarks=meanLandmarks ),train_patientsPaths)\n",
    "\n",
    "\n",
    "    toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "    toUp[0:numRows]=np.full(numRows, True)\n",
    "    colName= 'Nyul_'+seriesString\n",
    "    df[colName]=toUp \n",
    "\n",
    "#Important !!! set all labels that are non 0 to 1\n",
    "def changeLabelToOnes(path):\n",
    "    \"\"\"\n",
    "    as in the labels or meaningfull ones are greater then 0 so we need to process it and change any nymber grater to 0 to 1...\n",
    "    \"\"\"\n",
    "    if(path!= \" \" and path!=\"\"):\n",
    "        image1 = sitk.ReadImage(path)\n",
    "        data = sitk.GetArrayFromImage(image1)\n",
    "        data -= np.min(data)\n",
    "        data[data>= 1] = 1\n",
    "        #recreating image keeping relevant metadata\n",
    "        image = sitk.GetImageFromArray(data)\n",
    "        image.SetSpacing(image1.GetSpacing())\n",
    "        image.SetOrigin(image1.GetOrigin())\n",
    "        image.SetDirection(image1.GetDirection())\n",
    "        writer = sitk.ImageFileWriter()\n",
    "        writer.KeepOriginalImageUIDOn()\n",
    "        writer.SetFileName(path)\n",
    "        writer.Execute(image)   \n",
    "    \n",
    "def iterateAndchangeLabelToOnes(numRows,df):\n",
    "    \"\"\"\n",
    "    iterates over files from train_patientsPaths representing seriesString type of the study\n",
    "    and overwrites it with normalised biased corrected and standardised version\n",
    "    \"\"\"\n",
    "    #paralelize https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\n",
    "    train_patientsPaths=df['reSampledPath'].dropna().astype('str')[(df['reSampledPath'].str.len() >2)].to_numpy()[0:numRows]\n",
    "    numRows=len(train_patientsPaths)\n",
    "\n",
    "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "        pool.map(changeLabelToOnes,train_patientsPaths)\n",
    "    toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "    toUp[0:numRows]=np.full(numRows, True)\n",
    "    df['labels_to_one']=toUp    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         62.66250389 66.97992971 70.35550662 73.10728767 75.48056694\n",
      " 77.84429446 80.22498204 83.31652604 87.85813972 99.        ]\n",
      "/home/sliceruser/data/orig/10000/10000_1000000_t2w.mha\n",
      "/home/sliceruser/data/orig/10001/10001_1000001_t2w.mha\n",
      "/home/sliceruser/data/orig/10002/10002_1000002_t2w.mha\n",
      "/home/sliceruser/data/orig/10003/10003_1000003_t2w.mha\n",
      "/home/sliceruser/data/orig/10004/10004_1000004_t2w.mha\n",
      "[ 1.         62.98465087 67.20761231 70.40509665 73.00332193 75.34721194\n",
      " 77.54754587 80.01533088 82.94716876 87.69991626 99.        ]\n",
      "/home/sliceruser/data/orig/10000/10000_1000000_t2w.mha\n",
      "/home/sliceruser/data/orig/10002/10002_1000002_t2w.mha\n",
      "/home/sliceruser/data/orig/10001/10001_1000001_t2w.mha\n",
      "/home/sliceruser/data/orig/10003/10003_1000003_t2w.mha\n",
      "/home/sliceruser/data/orig/10004/10004_1000004_t2w.mha\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/jakub/New Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000010?line=5'>6</a>\u001b[0m iterateAndStandardize(\u001b[39m'\u001b[39m\u001b[39mt2w\u001b[39m\u001b[39m'\u001b[39m,numRows,df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000010?line=6'>7</a>\u001b[0m iterateAndStandardize(\u001b[39m'\u001b[39m\u001b[39madc\u001b[39m\u001b[39m'\u001b[39m,numRows,df)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000010?line=7'>8</a>\u001b[0m iterateAndStandardize(\u001b[39m'\u001b[39;49m\u001b[39mcor\u001b[39;49m\u001b[39m'\u001b[39;49m,numRows,df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000010?line=8'>9</a>\u001b[0m iterateAndStandardize(\u001b[39m'\u001b[39m\u001b[39mhbv\u001b[39m\u001b[39m'\u001b[39m,numRows,df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000010?line=9'>10</a>\u001b[0m iterateAndStandardize(\u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m,numRows,df)\n",
      "\u001b[1;32m/media/jakub/New Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb Cell 4'\u001b[0m in \u001b[0;36miterateAndStandardize\u001b[0;34m(seriesString, numRows, df)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=317'>318</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=318'>319</a>\u001b[0m \u001b[39miterates over files from train_patientsPaths representing seriesString type of the study\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=319'>320</a>\u001b[0m \u001b[39mand overwrites it with normalised biased corrected and standardised version\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=320'>321</a>\u001b[0m \u001b[39mnumRows - marks how many rows we want to process\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=321'>322</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=322'>323</a>\u001b[0m \u001b[39m#paralelize https://medium.com/python-supply/map-reduce-and-multiprocessing-8d432343f3e7\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=323'>324</a>\u001b[0m train_patientsPaths\u001b[39m=\u001b[39mdf[seriesString]\u001b[39m.\u001b[39mdropna()\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m'\u001b[39m)[(df[seriesString]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mlen() \u001b[39m>\u001b[39m\u001b[39m2\u001b[39m)]\u001b[39m.\u001b[39mto_numpy()[\u001b[39m0\u001b[39m:numRows]\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=324'>325</a>\u001b[0m \u001b[39m#print(train_patientsPaths)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/jakub/New%20Volume/slicerData/piCaiCode/preprocessing/MainPreprocessing.ipynb#ch0000007?line=325'>326</a>\u001b[0m \u001b[39mwith\u001b[39;00m mp\u001b[39m.\u001b[39mPool(processes \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mcpu_count()) \u001b[39mas\u001b[39;00m pool:\n",
      "File \u001b[0;32m~/Slicer/lib/Python/lib/python3.9/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[0;32m~/Slicer/lib/Python/lib/python3.9/site-packages/pandas/core/accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[0;32m--> 182\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[1;32m    183\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/Slicer/lib/Python/lib/python3.9/site-packages/pandas/core/strings/accessor.py:177\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m    175\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m~/Slicer/lib/Python/lib/python3.9/site-packages/pandas/core/strings/accessor.py:231\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    228\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39minfer_dtype(values, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
    "numRows=5\n",
    "iterateAndchangeLabelToOnes(numRows,df)\n",
    "# for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]:\n",
    "#     iterateAndStandardize(keyWord,numRows,df)\n",
    "iterateAndStandardize('t2w',numRows,df)\n",
    "iterateAndStandardize('adc',numRows,df)\n",
    "iterateAndStandardize('cor',numRows,df)\n",
    "iterateAndStandardize('hbv',numRows,df)\n",
    "iterateAndStandardize('sag',numRows,df)\n",
    "\n",
    "\n",
    "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       /home/sliceruser/data/orig/10000/10000_1000000...\n",
       "1       /home/sliceruser/data/orig/10001/10001_1000001...\n",
       "2       /home/sliceruser/data/orig/10002/10002_1000002...\n",
       "3       /home/sliceruser/data/orig/10003/10003_1000003...\n",
       "4       /home/sliceruser/data/orig/10004/10004_1000004...\n",
       "                              ...                        \n",
       "1495    /home/sliceruser/data/orig/11471/11471_1001495...\n",
       "1496    /home/sliceruser/data/orig/11472/11472_1001496...\n",
       "1497    /home/sliceruser/data/orig/11473/11473_1001497...\n",
       "1498    /home/sliceruser/data/orig/11474/11474_1001498...\n",
       "1499    /home/sliceruser/data/orig/11475/11475_1001499...\n",
       "Name: sag, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n",
    "\n",
    "# df['removeOutliersBiasFieldCorrect']=False\n",
    "# toUp=np.full(df.shape[0], False)#[0:3]=[True,True,True]\n",
    "# toUp[0:3]=[True,True,True]\n",
    "# df['removeOutliersBiasFieldCorrect']=toUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['removeOutliersBiasFieldCorrect'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
