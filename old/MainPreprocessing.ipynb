{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torchio as tio\n",
                "from torch.utils.data import DataLoader\n",
                "import os\n",
                "import SimpleITK as sitk\n",
                "from zipfile import ZipFile\n",
                "from zipfile import BadZipFile\n",
                "import dask.dataframe as dd\n",
                "import os\n",
                "import multiprocessing as mp\n",
                "import functools\n",
                "from functools import partial\n",
                "import Standardize\n",
                "import Resampling\n",
                "import utilsPreProcessing\n",
                "from utilsPreProcessing import write_to_modif_path \n",
                "from registration.elastixRegister import reg_adc_hbv_to_t2w\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "# if using a Jupyter notebook, includue:\n",
                "%matplotlib inline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## managment of files \n",
                "managment of files is done via managePicaiFiles.sh - create directories download and unpack files saves basic metadata and do simple metadata preprocessing\n",
                "sh managePicaiFiles.sh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
                "#currently We want only imagfes with associated masks\n",
                "df = df.loc[df['isAnyMissing'] ==False]\n",
                "df = df.loc[df['isAnythingInAnnotated']>0 ]\n",
                "# ignore all with deficient spacing\n",
                "for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]:    \n",
                "    colName=keyWord+ \"_spac_x\"\n",
                "    df = df.loc[df[colName]>0 ]\n",
                "#just for testing    \n",
                "df=df.sample(n = 4)#TODO remove\n",
                "\n",
                "df.to_csv('/home/sliceruser/data/metadata/processedMetaData_current.csv') \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Standarization\n",
                "primary preprocessing - removing ouliers, put values between 0 and 255 bias field correction Nyul standarization binarizing labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "fitting normalizer  t2w\n",
                        "fitting normalizer  adc\n",
                        "fitting normalizer  cor\n",
                        "fitting normalizer  hbv\n",
                        "fitting normalizer  sag\n"
                    ]
                }
            ],
            "source": [
                "import Standardize\n",
                "import pandas as pd\n",
                "trainedModelsBasicPath='/home/sliceruser/data/preprocess/standarizationModels'\n",
                "for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]:\n",
                "    Standardize.iterateAndStandardize(keyWord,df,trainedModelsBasicPath)   \n",
                "Standardize.iterateAndchangeLabelToOnes(df)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Setting spacing of adc and HBV to t2w so then there would be less resampling needed during registration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "origSize (84, 128, 19)\n",
                        "new_size (336, 512, 19)\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 3s 3s/step\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 10s 10s/step\n",
                        "origSize (84, 128, 19)\n",
                        "new_size (336, 512, 19)\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 2s 2s/step\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 10s 10s/step\n",
                        "origSize (84, 128, 19)\n",
                        "new_size (336, 512, 19)\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f038cc51dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
                        "1/1 [==============================] - 3s 3s/step\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f02e1633280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
                        "1/1 [==============================] - 10s 10s/step\n",
                        "origSize (84, 128, 19)\n",
                        "new_size (336, 512, 19)\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 3s 3s/step\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 10s 10s/step\n",
                        "origSize (84, 128, 19)\n",
                        "new_size (336, 512, 18)\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 3s 3s/step\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 10s 10s/step\n",
                        "origSize (84, 128, 19)\n",
                        "new_size (336, 512, 18)\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 3s 3s/step\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 10s 10s/step\n",
                        "origSize (256, 256, 27)\n",
                        "new_size (1023, 1023, 27)\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
                        "1/1 [==============================] - 20s 20s/step\n",
                        "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
                    ]
                }
            ],
            "source": [
                "def resample_adc_hbv_to_t2w(row,secondCol ):\n",
                "    pathT2w= row['t2w']\n",
                "    pathh= row[secondCol] \n",
                "    newPath = pathh.replace(\".mha\",\"_resmaplA.mha\" )\n",
                "    #we check weather resampling was already done if not we do the resampling\n",
                "    if(len(row[secondCol+'_resmaplA'] )<3):\n",
                "        imageT2W = sitk.ReadImage(pathT2w)\n",
                "        targetSpacing = imageT2W.GetSpacing()\n",
                "        try:\n",
                "            resampled = Resampling.resample_with_GAN(pathh,targetSpacing)\n",
                "        except:\n",
                "            print(\"error resampling\")\n",
                "        resampled = Resampling.resample_with_GAN(pathh,targetSpacing)\n",
                "\n",
                "        write_to_modif_path(resampled,pathh,\".mha\",\"_resmaplA.mha\" )\n",
                "    return newPath\n",
                "\n",
                "#needs to be on single thread as resampling GAN is acting on GPU\n",
                "# we save the metadata to main pandas data frame \n",
                "df[\"adc_resmaplA\"]=df.apply(lambda row : resample_adc_hbv_to_t2w(row, 'adc')   , axis = 1) \n",
                "df[\"hbv_resmaplA\"]=df.apply(lambda row : resample_adc_hbv_to_t2w(row, 'hbv')   , axis = 1) \n",
                "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n",
                "        \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Registration of adc and hb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/home/sliceruser/Slicer/NA-MIC/Extensions-30822/SlicerElastix/lib/Slicer-5.0/elastix -f /home/sliceruser/data/orig/10451/10451_1000459_t2w.mha -m /home/sliceruser/data/orig/10451/10451_1000459_adc.mha -out /home/sliceruser/data/orig/10451/10451_1000459_adc_for_adc -p /home/sliceruser/locTemp/piCaiCode/preprocessing/registration/parameters.txt\n",
                        "/home/sliceruser/Slicer/NA-MIC/Extensions-30822/SlicerElastix/lib/Slicer-5.0/elastix -f /home/sliceruser/data/orig/10804/10804_1000820_t2w.mha -m /home/sliceruser/data/orig/10804/10804_1000820_adc.mha -out /home/sliceruser/data/orig/10804/10804_1000820_adc_for_adc -p /home/sliceruser/locTemp/piCaiCode/preprocessing/registration/parameters.txt\n",
                        "/home/sliceruser/Slicer/NA-MIC/Extensions-30822/SlicerElastix/lib/Slicer-5.0/elastix -f /home/sliceruser/data/orig/11032/11032_1001052_t2w.mha -m /home/sliceruser/data/orig/11032/11032_1001052_adc.mha -out /home/sliceruser/data/orig/11032/11032_1001052_adc_for_adc -p /home/sliceruser/locTemp/piCaiCode/preprocessing/registration/parameters.txt\n",
                        "/home/sliceruser/Slicer/NA-MIC/Extensions-30822/SlicerElastix/lib/Slicer-5.0/elastix -f /home/sliceruser/data/orig/11194/11194_1001217_t2w.mha -m /home/sliceruser/data/orig/11194/11194_1001217_adc.mha -out /home/sliceruser/data/orig/11194/11194_1001217_adc_for_adc -p /home/sliceruser/locTemp/piCaiCode/preprocessing/registration/parameters.txt\n"
                    ]
                }
            ],
            "source": [
                "elacticPath='/home/sliceruser/Slicer/NA-MIC/Extensions-30822/SlicerElastix/lib/Slicer-5.0/elastix'\n",
                "reg_prop='/home/sliceruser/locTemp/piCaiCode/preprocessing/registration/parameters.txt'      \n",
                "        \n",
                "for keyWord in ['adc_resmaplA','hbv_resmaplA']:    \n",
                "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
                "        pool.map(partial(reg_adc_hbv_to_t2w,colName=keyWord,elacticPath=elacticPath,reg_prop=reg_prop )  ,list(df.iterrows()))    \n",
                " "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Getting all of the spacings "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'t2w_spac_x': (0.234375, 0.78125, 0.5, 0.5),\n",
                            " 't2w_spac_y': (0.234375, 0.78125, 0.5, 0.5),\n",
                            " 't2w_spac_z': (2.200000060372773, 5.000000066297424, 3.6, 3.0),\n",
                            " 'adc_spac_x': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
                            " 'adc_spac_y': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
                            " 'adc_spac_z': (2.999998832062665, 5.000000232855622, 4.0, 3.0),\n",
                            " 'cor_spac_x': (0.286756128073, 0.9765625, 0.6, 0.6),\n",
                            " 'cor_spac_y': (0.286756128073, 0.9765625, 0.6, 0.6),\n",
                            " 'cor_spac_z': (0.9999999999999636, 4.49999998188102, 2.7, 3.0),\n",
                            " 'hbv_spac_x': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
                            " 'hbv_spac_y': (0.859375, 2.5098040103912, 1.7, 2.0),\n",
                            " 'hbv_spac_z': (2.999998832062665, 5.000000232855622, 4.0, 3.0),\n",
                            " 'sag_spac_x': (0.254629641771, 0.78125, 0.5, 0.6),\n",
                            " 'sag_spac_y': (0.254629641771, 0.78125, 0.5, 0.6),\n",
                            " 'sag_spac_z': (2.999999882111563, 4.000404550206146, 3.5, 3.6)}"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df = pd.read_csv('/home/sliceruser/data/metadata/processedMetaData.csv')\n",
                "\n",
                "\"\"\"\n",
                "looking through all valid spacings (if it si invalid it goes below 0)\n",
                "and displaying minimal maximal and rounded mean spacing and median\n",
                "in my case median and mean values are close - and using the median values will lead to a bit less interpolations later\n",
                "\"\"\"\n",
                "spacingDict={}\n",
                "for keyWord in ['t2w','adc', 'cor','hbv','sag'  ]: \n",
                "    for addedKey in ['_spac_x','_spac_y','_spac_z']:   \n",
                "        colName = keyWord+addedKey\n",
                "        liist = list(filter(lambda it: it>0 ,df[colName].to_numpy() ))\n",
                "        minn=np.min(liist)                \n",
                "        maxx=np.max(liist)\n",
                "        meanRounded = round((minn+maxx)/2,1)\n",
                "        medianRounded = round(np.median(liist),1)\n",
                "        spacingDict[colName]=(minn,maxx,meanRounded,medianRounded)\n",
                "\n",
                "spacingDict\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now idea is to iteratively transform images into uniform spacing here i will choose median of the data as the target value and resample images to it to get uniform spacing\n",
                "To be more precise we will look at median t2w spacing and adjust t2w, adc and hbv to it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "targetSpacingg=(spacingDict['t2w_spac_x'][3],spacingDict['t2w_spac_y'][3],spacingDict['t2w_spac_z'][3])\n",
                "    \n",
                "    \n",
                "def resample_registered_to_given(row,colname,targetSpacing):\n",
                "    path=row[colname]\n",
                "    outPath = path.replace(\".mha\",\"_for_\"+colName)\n",
                "    registeredPath = outPath+\"/result.0.mha\"\n",
                "    newPath = path.replace(\".mha\",\"_medianSpac.mha\" )   \n",
                "    try:\n",
                "        resampled = Resampling.resample_with_GAN(registeredPath,targetSpacing)\n",
                "    except:\n",
                "        print(\"error resampling\")\n",
                "    resampled = Resampling.resample_with_GAN(registeredPath,targetSpacing)\n",
                "\n",
                "    write_to_modif_path(resampled,path,\".mha\",\"_medianSpac.mha\" )\n",
                "    return newPath\n",
                "\n",
                "\"\"\"\n",
                "registered images were already resampled now time for t2w and labels\n",
                "\"\"\"\n",
                "def resample_t2w(row,targetSpacing):\n",
                "    path=row['t2w']\n",
                "    newPath = path.replace(\".mha\",\"_medianSpac.mha\" )   \n",
                "    try:\n",
                "        resampled = Resampling.resample_with_GAN(registeredPath,targetSpacing)\n",
                "    except:\n",
                "        print(\"error resampling\")\n",
                "    resampled = Resampling.resample_with_GAN(registeredPath,targetSpacing)\n",
                "\n",
                "    write_to_modif_path(resampled,path,\".mha\",\"_medianSpac.mha\" )\n",
                "    return newPath    \n",
                "\n",
                "def resample_labels(row,targetSpacing):\n",
                "    path=row['reSampledPath']\n",
                "    newPath = path.replace(\".mha\",\"_medianSpac.mha\" )   \n",
                "    try:\n",
                "        resampled = Resampling.resample_label_with_GAN(registeredPath,targetSpacing)\n",
                "    except:\n",
                "        print(\"error resampling\")\n",
                "    resampled = Resampling.resample_label_with_GAN(registeredPath,targetSpacing)\n",
                "\n",
                "    write_to_modif_path(resampled,path,\".mha\",\"_medianSpac.mha\" )\n",
                "    return newPath        \n",
                "    \n",
                "\n",
                "#needs to be on single thread as resampling GAN is acting on GPU\n",
                "# we save the metadata to main pandas data frame \n",
                "df[\"adc_med_spac\"]=df.apply(lambda row : resample_registered_to_given(row, 'adc_resmaplA',targetSpacingg)   , axis = 1) \n",
                "df[\"hbv_med_spac\"]=df.apply(lambda row : resample_registered_to_given(row, 'hbv_resmaplA',targetSpacingg)   , axis = 1) \n",
                "df[\"t2w_med_spac\"]=df.apply(lambda row : resample_t2w(row,targetSpacingg)   , axis = 1) \n",
                "df[\"label_med_spac\"]=df.apply(lambda row : resample_labels(row,targetSpacingg)   , axis = 1) \n",
                "\n",
                "\n",
                "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n",
                "        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Now we need to retrieve the maximum dimensions of resampled images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_spatial_meta(row,colName):\n",
                "    #row=row[1]\n",
                "    patId=str(row['patient_id'])\n",
                "    path=str(row[colName])\n",
                "    if(len(path)>1):\n",
                "        image = sitk.ReadImage(path)\n",
                "        sizz= ifShortReturnMinus(image.GetSize(),patId,colName )\n",
                "        spac= ifShortReturnMinus(image.GetSpacing(),patId,colName)\n",
                "        orig= ifShortReturnMinus(image.GetOrigin(),patId,colName)\n",
                "        return list(sizz)+list(spac)+list(orig)\n",
                "    return [-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
                "for keyWord in ['t2w_med_spac']:    \n",
                "    resList=[]\n",
                "    with mp.Pool(processes = mp.cpu_count()) as pool:\n",
                "        resList=pool.map(partial(get_spatial_meta,colName=keyWord)  ,list(df.iterrows()))    \n",
                "    print(type(resList))    \n",
                "    df[keyWord+'_sizz_x']= list(map(lambda arr:arr[0], resList))    \n",
                "    df[keyWord+'_sizz_y']= list(map(lambda arr:arr[1], resList))    \n",
                "    df[keyWord+'_sizz_z']= list(map(lambda arr:arr[2], resList))\n",
                "    \n",
                "df.to_csv('/home/sliceruser/data/metadata/processedMetaData.csv') \n",
                "\n",
                "#getting maximum size - so one can pad to uniform size if needed (for example in validetion test set)\n",
                "median_spac_max_size_x = np.max(list(filter(lambda it: it>0 ,df['t2w_med_spac_sizz_x'].to_numpy() )))\n",
                "median_spac_max_size_y = np.max(list(filter(lambda it: it>0 ,df['t2w_med_spac_sizz_y'].to_numpy() )))\n",
                "median_spac_max_size_z = np.max(list(filter(lambda it: it>0 ,df['t2w_med_spac_sizz_z'].to_numpy() )))\n",
                "\n",
                "\n",
                "maxSize = (median_spac_max_size_x,median_spac_max_size_y,median_spac_max_size_z  )\n",
                "maxSize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#We will also experiment with some other spacing values so we will be able to set it as hyperparameter and check wheather for example low resolution will give better result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}