{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import monai\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import torchio as tio\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "monai.utils.set_determinism()\n",
    "\n",
    "print('Last run on', time.ctime())\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We define data module\n",
    "paths to files together with metadata are already in pandas dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiCaiDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, df):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "\n",
    "        self.subjects = None\n",
    "        self.train_subjects = None\n",
    "        self.val_subjects = None\n",
    "        self.test_subjects = None\n",
    "        \n",
    "        self.preprocess = None\n",
    "        self.transform = None\n",
    "        self.train_set = None\n",
    "        self.val_set = None\n",
    "        self.test_set = None\n",
    "    \n",
    "    def get_max_shape(self, subjects):\n",
    "        dataset = tio.SubjectsDataset(subjects)\n",
    "        shapes = np.array([s.spatial_shape for s in dataset])\n",
    "        return shapes.max(axis=0)\n",
    "    \n",
    "    def getSubjectDataFromDataFrame(row):\n",
    "        \"\"\"\n",
    "        given row from data frame prepares Subject object from it\n",
    "        \"\"\"\n",
    "        subject = tio.Subject(\n",
    "        #MRI images\n",
    "        adc=tio.ScalarImage(row['adc']),\n",
    "        cor=tio.ScalarImage(row['cor']),\n",
    "        hbv=tio.ScalarImage(row['hbv']),\n",
    "        sag=tio.ScalarImage(row['sag']),\n",
    "        t2w=tio.ScalarImage(row['t2w']),\n",
    "        anythingInMask=tio.ScalarImage(row['anythingInMask']),\n",
    "\n",
    "        #metadata from CSV\n",
    "        patient_id=tio.ScalarImage(row['patient_id']),\n",
    "        study_id=tio.ScalarImage(row['study_id']),\n",
    "        patient_age=tio.ScalarImage(row['patient_age']),\n",
    "        psa=tio.ScalarImage(row['psa']),\n",
    "        psad=tio.ScalarImage(row['psad']),\n",
    "        prostate_volume=tio.ScalarImage(row['prostate_volume']),\n",
    "        histopath_type=tio.ScalarImage(row['histopath_type']),\n",
    "        lesion_GS=tio.ScalarImage(row['lesion_GS']),\n",
    "        #resampled labels to t2w\n",
    "        label=tio.LabelMap(row['reSampledPath']),\n",
    "        diagnosis='negative')\n",
    "\n",
    "        return subject\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        dictList = self.df.to_dict().values()\n",
    "        self.subjects = map(lambda row: self.getSubjectDataFromDataFrame(row)   , dictList)\n",
    "\n",
    "\n",
    "        num_subjects = len(self.subjects )\n",
    "        # Random split into test train and validation\n",
    "        train_set, valid_set,test_set = torch.utils.data.random_split(self.subjects, [0.7, 0.15,0.15])\n",
    "    \n",
    "        self.train_subjects = train_set\n",
    "        self.val_subjects = valid_set\n",
    "        self.test_subjects = test_set\n",
    "        \n",
    "\n",
    "        # After\n",
    "        print('='*30)\n",
    "        print('Train data set:', len(train_set))\n",
    "        print('Test data set:', len(test_set))\n",
    "        print('Valid data set:', len(valid_set))\n",
    "\n",
    "\n",
    "    \n",
    "    def get_preprocessing_transform(self):\n",
    "        preprocess = tio.Compose([\n",
    "            tio.RescaleIntensity((-1, 1)),\n",
    "            tio.CropOrPad(self.get_max_shape(self.subjects)),\n",
    "            tio.EnsureShapeMultiple(8),  # for the U-Net\n",
    "            tio.OneHot(),\n",
    "        ])\n",
    "        return preprocess\n",
    "    \n",
    "    def get_augmentation_transform(self):\n",
    "        augment = tio.Compose([\n",
    "            tio.RandomAffine(),\n",
    "            # tio.RandomGamma(p=0.5),\n",
    "            # tio.RandomNoise(p=0.5),\n",
    "            tio.RandomMotion(p=0.1),\n",
    "            tio.RandomBiasField(p=0.25),\n",
    "        ])\n",
    "        return augment\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.preprocess = self.get_preprocessing_transform()\n",
    "        augment = self.get_augmentation_transform()\n",
    "        self.transform = tio.Compose([self.preprocess, augment])\n",
    "        self.train_set = tio.SubjectsDataset(self.train_subjects, transform=self.transform)\n",
    "        self.val_set = tio.SubjectsDataset(self.val_subjects, transform=self.preprocess)\n",
    "        self.test_set = tio.SubjectsDataset(self.test_subjects, transform=self.preprocess)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PiCaiDataModule(\n",
    "    df= df,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "data.prepare_data()\n",
    "data.setup()\n",
    "# print('Training:  ', len(data.train_set))\n",
    "# print('Validation: ', len(data.val_set))\n",
    "# print('Test:      ', len(data.test_set))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
